{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Model Application Development: RAG Chapter\n",
    "\n",
    "## I. What is RAG?\n",
    "\n",
    "### 1.1 Core Concepts of RAG\n",
    "Retrieval-Augmented Generation (RAG) is a technical framework that combines \"information retrieval\" with \"Large Language Models (LLMs)\". Simply put:\n",
    "- It enables large models to \"consult materials\" first (retrieve relevant information from your knowledge base) before answering questions\n",
    "- Then generate answers based on the retrieved materials to avoid \"fabrication\" (reduce hallucinations)\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/c41c00052949463692fe89f05751e26bb08bf0d1bce74d76889a1958b7baa954)\n",
    "\n",
    "### 1.2 Core Problems Solved by RAG\n",
    "Three major pain points of traditional large models addressed by RAG:\n",
    "- **Outdated knowledge**: Model training data has an expiration date, while RAG can access the latest knowledge in real-time\n",
    "- **Hallucination generation**: Models may fabricate incorrect information, but RAG grounds answers in real materials\n",
    "- **Domain limitations**: General models lack sufficient expertise in specialized fields, while RAG can connect to industry knowledge bases\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/091ca1c290fd479da526488c6a400c262c902e01c8bc4314bb1a1521eb6b507f)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/d87ee748c6c9416eb5d380585954c40d09ed166cf26244bc9169adefca3d1512)\n",
    "\n",
    "\n",
    "## II. Basic Workflow of RAG\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[User Query] --> B[Query Parsing]\n",
    "    B --> C[Retrieve from Knowledge Base]\n",
    "    C --> D[Obtain Relevant Documents]\n",
    "    D --> E[Splice into Prompt]\n",
    "    E --> F[LLM Generate Answer]\n",
    "    F --> G[Output Result]\n",
    "```\n",
    "\n",
    "Specific steps:\n",
    "1. **User Query**: For example, \"What new features does Wenxin Large Model 4.5 have?\"\n",
    "2. **Query Parsing**: Extract keywords \"Wenxin Large Model 4.5\" and \"new features\"\n",
    "3. **Retrieve from Knowledge Base**: Find content about Wenxin 4.5 in your documents\n",
    "4. **Obtain Relevant Documents**: Return 2-3 most relevant descriptions\n",
    "5. **Generate Prompt**: Combine the query and relevant documents into a prompt (e.g., \"Answer based on the following content: [Documents] Question: ...\")\n",
    "6. **Model Generation**: Locally deployed ERNIE-4.5-21B-A3B model generates answer based on the prompt\n",
    "7. **Output Result**: Return answer with supporting references\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/b6eef408e01d4becaca2efa6b839a3fef79a2790b53448c8a407cb395914fb1f)\n",
    "\n",
    "## III. Core Components of RAG Systems\n",
    "\n",
    "### 3.1 Three Core Modules\n",
    "1. **Knowledge Base**: Your private materials (documents, web pages, conversation records, etc.)\n",
    "2. **Retriever**: Quickly finds content related to the query from the knowledge base (key component is the \"vector database\")\n",
    "3. **Generator**: Locally deployed ERNIE-4.5-21B-A3B model (generates answers based on retrieved content)\n",
    "\n",
    "### 3.2 What is a Vector Database?\n",
    "- Converts text into \"numerical vectors\" (similar to \"mathematical fingerprints\" of text)\n",
    "- The more semantically similar two texts are, the closer their vector distance\n",
    "\n",
    "## IV. Selection of Wenxin Open-Source Models\n",
    "\n",
    "### 4.1 ERNIE-4.5 Model Series Specification Comparison Table\n",
    "\n",
    "| Model Series | Model Name | Total Parameters | Activated Parameters | Modality Support | Context Length | Main Use Cases | Deployment Scenario |\n",
    "|---------|---------|--------|---------|---------|-----------|---------|---------|\n",
    "| **A47B Large Scale** | ERNIE-4.5-300B-A47B-Base | 300B | 47B | Text | 128K | Pre-training Base | Cloud GPU Cluster |\n",
    "| | ERNIE-4.5-300B-A47B | 300B | 47B | Text | 128K | Instruction Following/Creative Generation | Cloud GPU Cluster |\n",
    "| | ERNIE-4.5-VL-424B-A47B-Base | 424B | 47B | Text+Vision | 128K | Multimodal Pre-training | Cloud GPU Cluster |\n",
    "| | ERNIE-4.5-VL-424B-A47B | 424B | 47B | Text+Vision | 128K | Image-Text Understanding/Generation | Cloud GPU Cluster |\n",
    "| **A3B Medium Scale** | ERNIE-4.5-21B-A3B-Base | 21B | 3B | Text | 128K | Pre-training Base | Single Machine with Multiple GPUs |\n",
    "| | **ERNIE-4.5-21B-A3B** | **21B** | **3B** | **Text** | **128K** | **Dialogue/Document Processing** | **Single Machine with Multiple GPUs** |\n",
    "| | ERNIE-4.5-VL-28B-A3B-Base | 28B | 3B | Text+Vision | 128K | Multimodal Pre-training | Single Machine with Multiple GPUs |\n",
    "| | ERNIE-4.5-VL-28B-A3B | 28B | 3B | Text+Vision | 128K | Lightweight Multimodal Applications | Single Machine with Multiple GPUs |\n",
    "| **0.3B Lightweight** | ERNIE-4.5-0.3B-Base | 0.3B | 0.3B | Text | 4K | Edge-side Pre-training | Mobile/Edge Devices |\n",
    "| | ERNIE-4.5-0.3B | 0.3B | 0.3B | Text | 4K | Real-time Dialogue | Mobile/Edge Devices |\n",
    "\n",
    "### 4.2 Model Specification Selection Strategy Table\n",
    "\n",
    "| Application Scenario | Recommended Model | Reason | Hardware Requirements | Inference Latency |\n",
    "|---------|---------|------|---------|---------|\n",
    "| **Complex Reasoning Tasks** | ERNIE-4.5-300B-A47B | Strongest reasoning capability | 8√óA100(80GB) | High |\n",
    "| **Creative Content Generation** | ERNIE-4.5-300B-A47B | Best creative performance | 8√óA100(80GB) | High |\n",
    "| **Multimodal Understanding** | ERNIE-4.5-VL-424B-A47B | Optimal image-text integration | 8√óA100(80GB) | High |\n",
    "| **Daily Dialogue Customer Service** | **ERNIE-4.5-21B-A3B** | **Balanced performance and cost** | **4√óV100(32GB)** | **Medium** |\n",
    "| **Document Information Extraction** | **ERNIE-4.5-21B-A3B** | **Sufficient understanding capability** | **4√óV100(32GB)** | **Medium** |\n",
    "| **Lightweight Multimodal** | ERNIE-4.5-VL-28B-A3B | Balanced image-text processing | 4√óV100(32GB) | Medium |\n",
    "| **Mobile Applications** | ERNIE-4.5-0.3B | Low latency, fast response | 1√óGPU/CPU | Low |\n",
    "| **Edge Computing** | ERNIE-4.5-0.3B | Minimal resource consumption | CPU/NPU | Low |\n",
    "\n",
    "**Reasons for selecting ERNIE-4.5-21B-A3B in this tutorial:**\n",
    "- Moderate parameter scale (21B total parameters, 3B activated parameters), achieving a balance between performance and resource consumption\n",
    "- Supports 128K long context, suitable for processing long documents\n",
    "- Strong dialogue and document processing capabilities, ideal for RAG application scenarios\n",
    "\n",
    "## V. Hands-on Implementation: Basic RAG System\n",
    "\n",
    "### 5.1 Environment Preparation\n",
    "#### 5.1.1 Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T12:29:41.720323Z",
     "iopub.status.busy": "2025-07-11T12:29:41.720001Z",
     "iopub.status.idle": "2025-07-11T12:29:43.179791Z",
     "shell.execute_reply": "2025-07-11T12:29:43.178969Z",
     "shell.execute_reply.started": "2025-07-11T12:29:41.720303Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T09:41:20.899848Z",
     "iopub.status.busy": "2025-07-11T09:41:20.899453Z",
     "iopub.status.idle": "2025-07-11T09:41:22.500888Z",
     "shell.execute_reply": "2025-07-11T09:41:22.499998Z",
     "shell.execute_reply.started": "2025-07-11T09:41:20.899820Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m pip install fastdeploy-gpu -i https://www.paddlepaddle.org.cn/packages/stable/fastdeploy-gpu-80_90/ --extra-index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 Model Download and Deployment\n",
    "**1. Download ERNIE-4.5-21B-A3B model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T09:41:24.963235Z",
     "iopub.status.busy": "2025-07-11T09:41:24.962840Z",
     "iopub.status.idle": "2025-07-11T09:41:27.805835Z",
     "shell.execute_reply": "2025-07-11T09:41:27.805159Z",
     "shell.execute_reply.started": "2025-07-11T09:41:24.963211Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Download model using AIStudio command\n",
    "!aistudio download --model PaddlePaddle/ERNIE-4.5-21B-A3B-Paddle --local_dir /home/aistudio/work/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Start model service**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "python -m fastdeploy.entrypoints.openai.api_server \\\n",
    "       --model /home/aistudio/work/models \\\n",
    "       --port 7000 \\\n",
    "       --metrics-port 7001 \\\n",
    "       --engine-worker-queue-port 7001 \\\n",
    "       --max-model-len 32768 \\\n",
    "       --max-num-seqs 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Test model connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T12:51:05.905972Z",
     "iopub.status.busy": "2025-07-11T12:51:05.905653Z",
     "iopub.status.idle": "2025-07-11T12:51:08.713126Z",
     "shell.execute_reply": "2025-07-11T12:51:08.712508Z",
     "shell.execute_reply.started": "2025-07-11T12:51:05.905952Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am an intelligent assistant developed jointly by Aistudio and Wenxin Large Model. My core function is to help users complete tasks such as knowledge Q&A, text creation, code debugging, and logical reasoning through natural language interaction. Whether it's academic research, daily consultation, or creative generation, you can communicate with me through text, and I will provide accurate, efficient, and context-appropriate solutions combining the technical advantages of multimodal large models.\r\n",
      "\r\n",
      "My design philosophy is \"understand needs, create value\". I support bilingual interaction in Chinese and English, and continuously optimize my capability boundaries through user feedback. If you have any specific needs (such as data analysis, code implementation, text polishing, etc.), please feel free to tell me, and I will do my best to assist!"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "host = \"0.0.0.0\"\n",
    "port = \"7000\"\n",
    "client = openai.Client(base_url=f\"http://{host}:{port}/v1\", api_key=\"null\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"null\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"You are an intelligent assistant developed by Aistudio and Wenxin Large Model. Please introduce yourself.\"}\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta:\n",
    "        print(chunk.choices[0].delta.content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Step 1: Document Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T12:51:18.504417Z",
     "iopub.status.busy": "2025-07-11T12:51:18.504074Z",
     "iopub.status.idle": "2025-07-11T12:51:19.642176Z",
     "shell.execute_reply": "2025-07-11T12:51:19.641607Z",
     "shell.execute_reply.started": "2025-07-11T12:51:18.504397Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\r\n",
      "2025-07-11 20:51:18,778 - jieba - DEBUG - Building prefix dict from the default dictionary ...\r\n",
      "Loading model from cache /tmp/jieba.cache\r\n",
      "2025-07-11 20:51:18,778 - jieba - DEBUG - Loading model from cache /tmp/jieba.cache\r\n",
      "Loading model cost 0.667 seconds.\r\n",
      "2025-07-11 20:51:19,445 - jieba - DEBUG - Loading model cost 0.667 seconds.\r\n",
      "Prefix dict has been built successfully.\r\n",
      "2025-07-11 20:51:19,445 - jieba - DEBUG - Prefix dict has been built successfully.\r\n",
      "2025-07-11 20:51:19,445 - DocumentProcessor - INFO - Starting processing 5 documents...\r\n",
      "Processing:   0%|                                         | 0/5 [00:00<?, ?it/s]2025-07-11 20:51:19,459 - DocumentProcessor - WARNING - Token count exceeded: 731/450\r\n",
      "2025-07-11 20:51:19,460 - DocumentProcessor - INFO - Successfully processed Witch Gameplay.txt => processed_data/processed_data.jsonl\r\n",
      "2025-07-11 20:51:19,469 - DocumentProcessor - WARNING - Token count exceeded: 566/450\r\n",
      "2025-07-11 20:51:19,471 - DocumentProcessor - INFO - Successfully processed Hunter Gameplay.txt => processed_data/processed_data.jsonl\r\n",
      "2025-07-11 20:51:19,487 - DocumentProcessor - INFO - Successfully processed Werewolf Gameplay.txt => processed_data/processed_data.jsonl\r\n",
      "2025-07-11 20:51:19,500 - DocumentProcessor - WARNING - Token count exceeded: 589/450\r\n",
      "2025-07-11 20:51:19,503 - DocumentProcessor - INFO - Successfully processed Villager Gameplay.txt => processed_data/processed_data.jsonl\r\n",
      "2025-07-11 20:51:19,515 - DocumentProcessor - WARNING - Token count exceeded: 793/450\r\n",
      "2025-07-11 20:51:19,516 - DocumentProcessor - INFO - Successfully processed Seer Gameplay.txt => processed_data/processed_data.jsonl\r\n",
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 73.20it/s]\r\n"
     ]
    }
   ],
   "source": [
    "!python document_processor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Step 2: Create Chroma Vector Database and Test Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T12:52:22.264985Z",
     "iopub.status.busy": "2025-07-11T12:52:22.264633Z",
     "iopub.status.idle": "2025-07-11T12:52:40.902470Z",
     "shell.execute_reply": "2025-07-11T12:52:40.901882Z",
     "shell.execute_reply.started": "2025-07-11T12:52:22.264966Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-11 20:52:23,046 - ChromaBuilder - INFO - üöÄ Starting to build Chroma knowledge base...\r\n",
      "2025-07-11 20:52:23,102 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\r\n",
      "2025-07-11 20:52:23,157 - ChromaBuilder - INFO - Using ChromaDB default embedding function\r\n",
      "2025-07-11 20:52:23,159 - ChromaBuilder - INFO - Chroma knowledge base initialized, data directory: ./chroma_db\r\n",
      "2025-07-11 20:52:23,159 - ChromaBuilder - INFO - Found 1 JSONL file\r\n",
      "Building knowledge base:   0%|                                         | 0/1 [00:00<?, ?it/s]2025-07-11 20:52:23,161 - ChromaBuilder - INFO - Starting to load data from processed_data/processed_data.jsonl...\r\n",
      "2025-07-11 20:52:23,162 - ChromaBuilder - ERROR - Failed to add documents in batch: Expected IDs to be unique, found duplicates of: 333dca46f32667f00ce6adff4a43f40e, 5cc95892c1ef7061451f7c769e9ed6f4, af452c146011ad354f5d81d27658f43f, 5c36d994c65c46cf1cc415bfda9172cb, b6551d553086493e832970c0dc8ad843, fcad930fca8e3ff39bbb9dbc46c1c066, cbbfb58e7d38dd3efb5e36be576b8c00, 1f71df488b3bee56006bd913a95928e4, fbc3e8f5726c1f7349f16493519970f4 in add.\r\n",
      "2025-07-11 20:52:34,133 - ChromaBuilder - INFO - Data loading completed, collection 'data' has 9 documents\r\n",
      "Building knowledge base: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:10<00:00, 10.97s/it]\r\n",
      "2025-07-11 20:52:34,134 - ChromaBuilder - INFO - üéâ Knowledge base construction completed! Total 9 documents loaded\r\n",
      "2025-07-11 20:52:34,138 - ChromaBuilder - INFO - üìä Collection statistics:\r\n",
      "2025-07-11 20:52:34,138 - ChromaBuilder - INFO -   knowledge_base: 0 documents\r\n",
      "2025-07-11 20:52:34,138 - ChromaBuilder - INFO -   data: 9 documents\r\n",
      "2025-07-11 20:52:34,138 - ChromaBuilder - INFO - ‚úÖ Chroma database connection is normal, total 2 collections\r\n",
      "2025-07-11 20:52:34,139 - ChromaBuilder - INFO -   üìö knowledge_base: 0 documents\r\n",
      "2025-07-11 20:52:34,139 - ChromaBuilder - INFO -   üìö data: 9 documents\r\n",
      "2025-07-11 20:52:34,141 - ChromaBuilder - INFO - üîç Test search: Advanced Witch Gameplay (in collection: data)\r\n",
      "2025-07-11 20:52:34,349 - ChromaBuilder - INFO - Query 'Advanced Witch Gameplay...' returned 3 results\r\n",
      "2025-07-11 20:52:34,349 - ChromaBuilder - INFO - Search results:\r\n",
      "2025-07-11 20:52:34,349 - ChromaBuilder - INFO -   Result 1 (Similarity:-0.033): Villager Gameplay Knowledge Base (Advanced Analysis Version)\r\n",
      "Core goal: Build a multi-dimensional perspective analysis network through information integration and logical modeling, assistÁ•ûËÅåÁ≤æÂáÜÂΩíÁãº, while avoiding becoming a round-consuming item....\r\n",
      "2025-07-11 20:52:34,349 - ChromaBuilder - INFO -     Source: knowledge_data/Villager Gameplay.txt\r\n",
      "2025-07-11 20:52:34,349 - ChromaBuilder - INFO -   Result 2 (Similarity:-0.220): If you keep me, the Witch will poison No.5 tonight\" ‚Üí Werewolf No.5 is forced toËá™ÁàÜ\r\n",
      "\r\n",
      "Case 2: Endgame Double Kill\r\n",
      "\r\n",
      "Configuration: 1 werewolf, 1 hunter, 1 villager. Hunter is killed ‚Üí takes werewolf with him, good camp wins\r\n",
      "\r\n",
      "Case 3: Probability Fraud\r\n",
      "\r\n",
      "Hunter disguises as villager to protect fake seer, luring werewolf team to kill real seer ‚Üí shoots to take down fake seer...\r\n",
      "2025-07-11 20:52:34,349 - ChromaBuilder - INFO -     Source: knowledge_data/Hunter Gameplay.txt\r\n",
      "2025-07-11 20:52:34,350 - ChromaBuilder - INFO -   Result 3 (Similarity:-0.273): Werewolf Gameplay Knowledge Base (Advanced Tactics Version)\r\n",
      "Core goal: Destroy the good camp's information chain through logical deception, identity disguise, and precise night killing, achieving victory by eliminating all gods or all villagers. I. Night Tactical System\r\n",
      "1. Killing Decision Model\r\n",
      "Round\tPriority Target\tTactical Logic\r\n",
      "First night\tWitch/Seer...\r\n",
      "2025-07-11 20:52:34,350 - ChromaBuilder - INFO -     Source: knowledge_data/Werewolf Gameplay.txt\r\n",
      "2025-07-11 20:52:34,350 - ChromaBuilder - INFO - ‚úÖ Knowledge base construction and testing completed!"
     ]
    }
   ],
   "source": [
    "!python chroma_builder.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Step 3: Call Local ERNIE-4.5 Model to Generate Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T12:53:35.573507Z",
     "iopub.status.busy": "2025-07-11T12:53:35.573152Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\r\n",
      "2025-07-11 20:53:35,694 - jieba - DEBUG - Building prefix dict from the default dictionary ...\r\n",
      "Loading model from cache /tmp/jieba.cache\r\n",
      "2025-07-11 20:53:35,695 - jieba - DEBUG - Loading model from cache /tmp/jieba.cache\r\n",
      "Loading model cost 0.677 seconds.\r\n",
      "2025-07-11 20:53:36,372 - jieba - DEBUG - Loading model cost 0.677 seconds.\r\n",
      "Prefix dict has been built successfully.\r\n",
      "2025-07-11 20:53:36,376 - jieba - DEBUG - Prefix dict has been built successfully.\r\n",
      "2025-07-11 20:53:36,952 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\r\n",
      "2025-07-11 20:53:37,009 - ChromaBuilder - INFO - Using ChromaDB default embedding function\r\n",
      "2025-07-11 20:53:37,012 - ChromaBuilder - INFO - Chroma knowledge base initialized, data directory: ./chroma_db\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing RAG Intelligent Question Answering System...\r\n",
      "‚úÖ Knowledge base loaded, total 9 documents\r\n",
      "\r\n",
      "üéØ RAG Intelligent Question Answering System - Streaming Dialogue Mode\r\n",
      "‚ú® Based on local ERNIE-4.5-21B-A3B model\r\n",
      "üí° Enter questions to start dialogue, enter 'quit' or 'exit' to quit\r\n",
      "üí° Enter 'stats' to view knowledge base statistics\r\n",
      "üí° Enter 'clear' to clear screen\r\n",
      "============================================================\r\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\r\n",
      "‚ùì Please enter your question:  Werewolf\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "üîç Searching for relevant materials...\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 20:53:52,587 - ChromaBuilder - INFO - Query 'Werewolf...' returned 3 results\r\n",
      "2025-07-11 20:53:52,599 - httpx - INFO - HTTP Request: POST http://0.0.0.0:7000/v1/chat/completions \"HTTP/1.1 200 OK\"\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Found 3 relevant materials, sources: knowledge_data/Hunter Gameplay.txt, knowledge_data/Werewolf Gameplay.txt\r\n",
      "\r\n",
      "ü§ñ ERNIE-4.5 is thinking\r\n",
      "‚ú® Answer: Based on the provided reference materials, here is a detailed answer about werewolf gameplay:\r\n",
      "\r\n",
      "### Core Strategies and Techniques for Werewolf Gameplay\r\n",
      "\r\n",
      "1. **Round Pressure Response Table**  \r\n",
      "   - **3 werewolves**: Aggressively fake identity, quickly compress the information space of good players, confuse the good camp through frequent speeches or misleading information.  \r\n",
      "   - **2 werewolves**: 1 werewolf feigns allegiance + 1 werewolf hides deeply. The feigning werewolf pretends to be a good player to vote out key special roles, while the deep-hiding werewolf waits for the right moment.  \r\n",
      "   - **1 werewolf**: Hide completely, exploit logical loopholes of good players to vote out key special roles (such as seer, witch), avoid early exposure.  \r\n",
      "   *Source: Reference 3 (Werewolf Gameplay.txt)*\r\n",
      "\r\n",
      "2. **Advanced Thinking Models**  \r\n",
      "   - **Reverse Thinking Deduction**: Assume you are a good player, build logical chains to ensure consistent speeches, predict and interfere with the real seer's inspection path.  \r\n",
      "   - **Bayesian Probability Calculation**: Dynamically estimate the success rate of eliminating all gods or all villagers based on the number of survivors (e.g., with 4 villagers, 2 gods, and 1 werewolf remaining, 3 kills are needed to eliminate all villagers, 2 kills to eliminate all gods).  \r\n",
      "   - **Information Pollution Techniques**:  \r\n",
      "     - **False Alliance Relationships**: Deliberately protect a good player (e.g., \"Player X is definitely good, I guarantee it\"), luring the witch to suspect X.  \r\n",
      "     - **Timeline Tampering**: Fabricate night information (e.g., \"If I were the witch, I would have poisoned Player Y last night\"), implying Y may pose a poisoning threat.  \r\n",
      "   *Source: Reference 3 (Werewolf Gameplay.txt)*\r\n",
      "\r\n",
      "3. **Taboos and Fault-Tolerance Mechanisms**  \r\n",
      "   - **Absolutely Forbidden Behaviors**:  \r\n",
      "     - Night killing violates priorities (e.g., killing a villager in the endgame while missing the hunter).  \r\n",
      "     - Faking as seer but falsely accusing the real seer.  \r\n",
      "     - Feigning werewolf exposing too early leading to team collapse.  \r\n",
      "   - **Fault-Tolerance Repair Solutions**:  \r\n",
      "     - **Killing a guarded target**: Claim the witch used a potion the next day to divert attention.  \r\n",
      "     - **Fake identity exposed**: Other werewolves immediately feign allegiance, accusing the exposed one of \"werewolf framing\".  \r\n",
      "     - **Accidentally killing a key special role**: Fabricate a logical chain in the last words to frame them as a \"self-killing werewolf\".  \r\n",
      "   *Source: Reference 3 (Werewolf Gameplay.txt)*\r\n",
      "\r\n",
      "4. **Practical Case Library**  \r\n",
      "   - **Self-kill + Feigned Allegiance Combination**:  \r\n",
      "     - Werewolf A self-kills on the first night, witch saves them ‚Üí Werewolf B fakes identity and confirms Werewolf A as good during the day ‚Üí Real seer inspects Werewolf A and confirms as werewolf ‚Üí Werewolf C feigns allegiance to the real seer, ultimately framing the real seer as a werewolf.  \r\n",
      "   - **Endgame Balanced Faction Elimination**:  \r\n",
      "     - 1 werewolf (Werewolf D), 1 hunter, 2 villagers remaining ‚Üí Werewolf D votes out a villager during the day, kills the hunter at night, triggering victory by eliminating all villagers.  \r\n",
      "   *Source: Reference 3 (Werewolf Gameplay.txt)*\r\n",
      "\r\n",
      "### Impact of Hunter Gameplay on Werewolves\r\n",
      "\r\n",
      "- **Hunter Disguised as Villager**: Hunters may pretend to be villagers to protect fake seers, luring the werewolf team to kill the real seer, then shoot to take down the fake seer.  \r\n",
      "- **Endgame Double Kill Case**: If the hunter is killed with their ability intact, they can take a werewolf down, securing victory for the good camp.  \r\n",
      "*Source: Reference 1 (Hunter Gameplay.txt)*\r\n",
      "\r\n",
      "### Summary\r\n",
      "\r\n",
      "Werewolf gameplay requires combining round pressure response, advanced thinking models (such as reverse deduction, probability calculation), information pollution techniques, and taboo management. In actual play, flexibly use strategies like self-killing, feigning allegiance, and identity faking, and repair mistakes through fault-tolerance mechanisms. Cases like hunter disguise further reflect the complexity and strategic depth of the werewolf faction.\r\n",
      "\r\n",
      "üìñ Reference sources: knowledge_data/Hunter Gameplay.txt, knowledge_data/Werewolf Gameplay.txt\r\n",
      "------------------------------------------------------------\r\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\r\n",
      "‚ùì Please enter your question:  Villager gameplay\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "üîç Searching for relevant materials...\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 20:54:41,438 - ChromaBuilder - INFO - Query 'Villager gameplay...' returned 3 results\r\n",
      "2025-07-11 20:54:41,447 - httpx - INFO - HTTP Request: POST http://0.0.0.0:7000/v1/chat/completions \"HTTP/1.1 200 OK\"\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Found 3 relevant materials, sources: knowledge_data/Hunter Gameplay.txt, knowledge_data/Villager Gameplay.txt, knowledge_data/Werewolf Gameplay.txt\r\n",
      "\r\n",
      "ü§ñ ERNIE-4.5 is thinking\r\n",
      "‚ú® Answer: Based on the provided reference materials, the core goal of villager gameplay is to build a multi-dimensional perspective analysis network through information integration and logical modeling, assist special roles in accurately identifying werewolves, while avoiding becoming a round-consuming item. However, the reference materials do not directly provide specific operational strategies or detailed steps for villager gameplay.\r\n",
      "\r\n",
      "In Werewolf game, villagers (ordinary players) typically do not have special abilities but can assist special role players (such as seers, witches, hunters, etc.) in reasoning by observing other players' behaviors, speeches, and voting patterns. Here are some general strategies that villager players might adopt:\r\n",
      "\r\n",
      "1. **Observation and Recording**: Villager players should closely monitor other players' speeches and behaviors, recording any suspicious or abnormal actions. This helps make more accurate judgments in subsequent voting.\r\n",
      "\r\n",
      "2. **Assisting Special Roles**: Villager players can try to establish connections with special role players, providing information or supporting their judgments. For example, if a seer accuses a player, villager players can ask the seer for reasons and attempt to verify the authenticity of this information.\r\n",
      "\r\n",
      "3. **Avoiding Exposure**: Villager players should try to avoid revealing their identity or stance in the game to prevent becoming targets of the werewolf team. This includes maintaining neutrality in speeches and avoiding overly aggressive remarks.\r\n",
      "\r\n",
      "4. **Rational Voting**: During voting, villager players should vote based on observed information and logical reasoning, avoiding blind following or impulsive voting.\r\n",
      "\r\n",
      "5. **Team Cooperation**: Villager players should maintain close cooperation with other good players to jointly counter the werewolf team. This includes sharing information and supporting each other during the game.\r\n",
      "\r\n",
      "It should be noted that the above strategies are derived from the general rules and logical reasoning of Werewolf game, and are not specific operation guides for villager gameplay. Since the reference materials do not provide specific strategies for villager gameplay, the above answer is only a general suggestion.\r\n",
      "\r\n",
      "Source: Villager Gameplay Knowledge Base (Advanced Analysis Version), which emphasizes that the core goal of villager gameplay is to assist special roles in identifying werewolves through information integration and logical modeling, but does not provide specific operational strategies.\r\n",
      "\r\n",
      "üìñ Reference sources: knowledge_data/Hunter Gameplay.txt, knowledge_data/Villager Gameplay.txt, knowledge_data/Werewolf Gameplay.txt\r\n",
      "------------------------------------------------------------\r\n"
     ]
    }
   ],
   "source": [
    "# rag_example.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import openai\n",
    "from document_processor import DocumentProcessor\n",
    "from chroma_builder import ChromaKnowledgeBase, build_knowledge_base_from_processed_data\n",
    "\n",
    "# Configure logging - only show errors and warnings\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format='%(levelname)s: %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"RAGExample\")\n",
    "\n",
    "class ERNIERAGSystem:\n",
    "    def __init__(self, \n",
    "                 ernie_host: str = \"0.0.0.0\",\n",
    "                 ernie_port: str = \"7000\",\n",
    "                 chroma_db_dir: str = \"./chroma_db\",\n",
    "                 embedding_model: str = \"default\"):\n",
    "        \"\"\"\n",
    "        Initialize RAG system\n",
    "        \n",
    "        Args:\n",
    "            ernie_host: ERNIE model service host\n",
    "            ernie_port: ERNIE model service port\n",
    "            chroma_db_dir: Chroma database directory\n",
    "            embedding_model: Embedding model type\n",
    "        \"\"\"\n",
    "        # Initialize local ERNIE model client\n",
    "        self.ernie_client = openai.Client(\n",
    "            base_url=f\"http://{ernie_host}:{ernie_port}/v1\", \n",
    "            api_key=\"null\"\n",
    "        )\n",
    "        \n",
    "        # Initialize knowledge base\n",
    "        self.knowledge_base = ChromaKnowledgeBase(\n",
    "            persist_directory=chroma_db_dir,\n",
    "            embedding_model=embedding_model\n",
    "        )\n",
    "\n",
    "    def retrieve_relevant_docs(self, question: str, top_k: int = 3, \n",
    "                             collection_name: str = None) -> list:\n",
    "        \"\"\"Retrieve relevant documents\"\"\"\n",
    "        # If no collection name specified, automatically select collection with data\n",
    "        if collection_name is None:\n",
    "            stats = self.knowledge_base.get_collection_stats()\n",
    "            for name, info in stats.items():\n",
    "                if info['document_count'] > 0:\n",
    "                    collection_name = name\n",
    "                    break\n",
    "            \n",
    "            if collection_name is None:\n",
    "                return []\n",
    "        \n",
    "        results = self.knowledge_base.search_knowledge(\n",
    "            query=question,\n",
    "            collection_name=collection_name,\n",
    "            n_results=top_k\n",
    "        )\n",
    "        \n",
    "        documents = results[\"documents\"][0]\n",
    "        metadatas = results[\"metadatas\"][0]\n",
    "        distances = results[\"distances\"][0]\n",
    "        \n",
    "        # Format retrieval results\n",
    "        retrieved_docs = []\n",
    "        for i, (doc, metadata, distance) in enumerate(zip(documents, metadatas, distances)):\n",
    "            similarity = 1 - distance\n",
    "            retrieved_docs.append({\n",
    "                \"text\": doc,\n",
    "                \"metadata\": metadata,\n",
    "                \"similarity\": similarity,\n",
    "                \"rank\": i + 1\n",
    "            })\n",
    "        \n",
    "        return retrieved_docs\n",
    "\n",
    "    def generate_answer_stream(self, question: str, context_docs: list, \n",
    "                             max_tokens: int = 1000, temperature: float = 0.7):\n",
    "        \"\"\"Generate answer stream using ERNIE model\"\"\"\n",
    "        \n",
    "        # Build context\n",
    "        context_parts = []\n",
    "        for i, doc_info in enumerate(context_docs):\n",
    "            source = doc_info[\"metadata\"].get(\"source\", \"Unknown source\")\n",
    "            similarity = doc_info[\"similarity\"]\n",
    "            context_parts.append(f\"Reference {i+1} (Similarity:{similarity:.3f}, Source:{source}):\\n{doc_info['text']}\")\n",
    "        \n",
    "        context_text = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt = f\"\"\"Please answer the question based on the following reference materials. If the reference materials do not contain relevant information, clearly state \"Cannot fully answer this question based on the provided reference materials\".\n",
    "\n",
    "Reference materials:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide an accurate and detailed answer based on the above reference materials, and indicate information sources at appropriate positions:\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Call local ERNIE model (streaming)\n",
    "            response = self.ernie_client.chat.completions.create(\n",
    "                model=\"null\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                stream=True  # Enable streaming output\n",
    "            )\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to call ERNIE model: {e}\")\n",
    "            return None\n",
    "\n",
    "    def ask_stream(self, question: str, top_k: int = 3, collection_name: str = None):\n",
    "        \"\"\"Streaming RAG question answering process\"\"\"\n",
    "        print(f\"\\nüîç Searching for relevant materials...\")\n",
    "        \n",
    "        # 1. Retrieve relevant documents\n",
    "        retrieved_docs = self.retrieve_relevant_docs(question, top_k, collection_name)\n",
    "        \n",
    "        if not retrieved_docs:\n",
    "            print(\"üíî Sorry, no relevant reference materials were found to answer your question.\")\n",
    "            return\n",
    "        \n",
    "        # Display retrieved materials information\n",
    "        sources = list(set([doc[\"metadata\"].get(\"source\", \"Unknown source\") for doc in retrieved_docs]))\n",
    "        print(f\"üìö Found {len(retrieved_docs)} relevant materials, sources: {', '.join(sources)}\")\n",
    "        print(f\"\\nü§ñ ERNIE-4.5 is thinking\")\n",
    "        \n",
    "        # 2. Generate answer stream\n",
    "        response_stream = self.generate_answer_stream(question, retrieved_docs)\n",
    "        \n",
    "        if response_stream is None:\n",
    "            print(\"‚ùå Error occurred while generating answer. Please check if ERNIE model service is running normally.\")\n",
    "            return\n",
    "        \n",
    "        print(\"‚ú® Answer: \", end=\"\", flush=True)\n",
    "        \n",
    "        # Process streaming response\n",
    "        full_answer = \"\"\n",
    "        try:\n",
    "            for chunk in response_stream:\n",
    "                if chunk.choices[0].delta and chunk.choices[0].delta.content:\n",
    "                    content = chunk.choices[0].delta.content\n",
    "                    print(content, end=\"\", flush=True)\n",
    "                    full_answer += content\n",
    "            \n",
    "            print(f\"\\n\\nüìñ Reference sources: {', '.join(sources)}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error occurred during streaming output: {e}\")\n",
    "            \n",
    "        return full_answer\n",
    "\n",
    "\n",
    "\n",
    "def interactive_rag_chat(rag_system: ERNIERAGSystem):\n",
    "    \"\"\"Interactive RAG streaming dialogue\"\"\"\n",
    "    print(\"\\nüéØ RAG Intelligent Question Answering System - Streaming Dialogue Mode\")\n",
    "    print(\"‚ú® Based on local ERNIE-4.5-21B-A3B model\")\n",
    "    print(\"üí° Enter questions to start dialogue, enter 'quit' or 'exit' to quit\")\n",
    "    print(\"üí° Enter 'stats' to view knowledge base statistics\")\n",
    "    print(\"üí° Enter 'clear' to clear screen\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            question = input(\"\\n‚ùì Please enter your question: \").strip()\n",
    "            \n",
    "            if question.lower() in ['quit', 'exit', 'ÈÄÄÂá∫', 'q']:\n",
    "                print(\"\\nüëã Thank you for using RAG Intelligent Question Answering System, goodbye!\")\n",
    "                break\n",
    "            elif question.lower() == 'stats':\n",
    "                stats = rag_system.knowledge_base.get_collection_stats()\n",
    "                print(\"\\nüìä Knowledge base statistics:\")\n",
    "                total_docs = 0\n",
    "                for name, info in stats.items():\n",
    "                    count = info['document_count']\n",
    "                    total_docs += count\n",
    "                    if count > 0:\n",
    "                        print(f\"  ‚úÖ {name}: {count} documents\")\n",
    "                    else:\n",
    "                        print(f\"  ‚ö™ {name}: {count} documents\")\n",
    "                print(f\"üìà Total: {total_docs} documents\")\n",
    "                continue\n",
    "            elif question.lower() == 'clear':\n",
    "                import os\n",
    "                os.system('cls' if os.name == 'nt' else 'clear')\n",
    "                print(\"üéØ RAG Intelligent Question Answering System - Streaming Dialogue Mode\")\n",
    "                continue\n",
    "            elif not question:\n",
    "                print(\"‚ö†Ô∏è Please enter a valid question\")\n",
    "                continue\n",
    "            \n",
    "            # Perform RAG streaming question answering\n",
    "            rag_system.ask_stream(question, top_k=3)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nüëã Program interrupted, goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error occurred: {e}\")\n",
    "            print(\"üí° Please check if ERNIE model service is running normally\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main program entry\"\"\"\n",
    "    print(\"üöÄ Initializing RAG Intelligent Question Answering System...\")\n",
    "    \n",
    "    try:\n",
    "        # Directly initialize RAG system\n",
    "        rag_system = ERNIERAGSystem(\n",
    "            ernie_host=\"0.0.0.0\",\n",
    "            ernie_port=\"7000\",\n",
    "            chroma_db_dir=\"./chroma_db\",\n",
    "            embedding_model=\"default\"\n",
    "        )\n",
    "        \n",
    "        # Check if knowledge base has data\n",
    "        stats = rag_system.knowledge_base.get_collection_stats()\n",
    "        total_docs = sum(info['document_count'] for info in stats.values())\n",
    "        \n",
    "        if total_docs == 0:\n",
    "            print(\"‚ö†Ô∏è Warning: No data in knowledge base!\")\n",
    "            print(\"üí° Please run the following commands to build knowledge base first:\")\n",
    "            print(\"   python document_processor.py\")\n",
    "            print(\"   python chroma_builder.py\")\n",
    "            return\n",
    "        \n",
    "        print(f\"‚úÖ Knowledge base loaded, total {total_docs} documents\")\n",
    "        \n",
    "        # Start interactive dialogue\n",
    "        interactive_rag_chat(rag_system)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå System initialization failed: {e}\")\n",
    "        print(\"üí° Please check:\")\n",
    "        print(\"  1. Whether ERNIE model service is running on port 7000\")\n",
    "        print(\"  2. Whether knowledge base directory ./chroma_db exists\")\n",
    "        print(\"  3. Whether relevant dependencies are installed correctly\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Industry Application Scenarios of RAG\n",
    "\n",
    "### 6.1 Enterprise Customer Service\n",
    "- Use RAG to connect product manuals and after-sales procedures, enabling customer service robots to call up the latest materials in real-time to answer user questions, reducing manual intervention\n",
    "- **Localization advantage**: Enterprise data never leaves the company, ensuring data security\n",
    "\n",
    "### 6.2 Medical Assistance\n",
    "- Doctors input patient symptoms, RAG retrieves the latest clinical guidelines and cases, assisting doctors in judging possible causes (requires professional review)\n",
    "- **Localization advantage**: Patient privacy data is processed entirely locally\n",
    "\n",
    "### 6.3 Educational Tutoring\n",
    "- When students ask math questions, RAG finds relevant knowledge points from textbooks and exercise sets, and the large model explains based on textbook content, ensuring synchronization with teaching\n",
    "- **Localization advantage**: Teaching content is controllable, no need to worry about network connection issues\n",
    "\n",
    "### 6.4 Legal Retrieval\n",
    "- Lawyers input case details, RAG retrieves relevant laws and precedents, and the large model generates legal analysis, improving retrieval efficiency\n",
    "- **Localization advantage**: Case information remains confidential, meeting data security requirements of the legal industry\n",
    "\n",
    "## VII. Optimization Tips for Beginners\n",
    "\n",
    "1. **Text Chunking Optimization**: When splitting long documents, split by \"semantic completeness\" (e.g., by paragraphs) to avoid cutting off sentences\n",
    "   ```python\n",
    "   from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "   text_splitter = RecursiveCharacterTextSplitter(\n",
    "       chunk_size=500, \n",
    "       chunk_overlap=50,\n",
    "       separators=[\"\\n\\n\", \"\\n\", \"„ÄÇ\", \"ÔºÅ\", \"Ôºü\", \"Ôºõ\"]\n",
    "   )\n",
    "   chunks = text_splitter.split_text(long_document)\n",
    "   ```\n",
    "\n",
    "2. **Prompt Optimization**: Clearly tell the model to \"only answer based on the provided materials\"\n",
    "   ```\n",
    "   Please answer strictly based on the following materials, do not mention information outside the materials. If materials are insufficient, directly say \"Cannot answer this question based on the provided materials\".\n",
    "   Materials: {context}\n",
    "   Question: {question}\n",
    "   ```\n",
    "\n",
    "Through the above steps, you have mastered the core logic and complete implementation of the RAG system based on the locally deployed ERNIE-4.5-21B-A3B model. This solution ensures data security (fully localized) while providing powerful knowledge question-answering capabilities, making it an ideal choice for enterprise-level RAG applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}