{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【ERNIE-4.5-21B-A3B】Invoice Process Automation System Based on Local Deployment of PP-OCR and ERNIE-4.5-21B-A3B Models\n",
    "\n",
    "\n",
    "## Selection of Wenxin Open-Source Models\n",
    "\n",
    "This project focuses on the local deployment requirements of enterprise-level invoice processing scenarios. During the model selection phase, a comprehensive evaluation of the ERNIE-4.5 series models was conducted, and **ERNIE-4.5-21B-A3B-Paddle** was ultimately chosen as the core large model, combined with PP-OCR to achieve full-process local deployment.\n",
    "\n",
    "\n",
    "### **ERNIE-4.5 Model Series Specification Comparison Table**\n",
    "\n",
    "| Model Series | Model Name | Total Parameters | Activated Parameters | Modality Support | Context Length | Main Usage | Deployment Scenario |\n",
    "|---------|---------|--------|---------|---------|-----------|---------|---------|\n",
    "| **A47B Large Scale** | ERNIE-4.5-300B-A47B-Base | 300B | 47B | Text | 128K | Pre-training Base | Cloud GPU Cluster |\n",
    "| | ERNIE-4.5-300B-A47B | 300B | 47B | Text | 128K | Instruction Following/Creative Generation | Cloud GPU Cluster |\n",
    "| | ERNIE-4.5-VL-424B-A47B-Base | 424B | 47B | Text+Vision | 128K | Multimodal Pre-training | Cloud GPU Cluster |\n",
    "| | ERNIE-4.5-VL-424B-A47B | 424B | 47B | Text+Vision | 128K | Image-Text Understanding/Generation | Cloud GPU Cluster |\n",
    "| **A3B Medium Scale** | ERNIE-4.5-21B-A3B-Base | 21B | 3B | Text | 128K | Pre-training Base | Single-machine Multi-GPU |\n",
    "| | ERNIE-4.5-21B-A3B | 21B | 3B | Text | 128K | Dialogue/Document Processing | Single-machine Multi-GPU |\n",
    "| | ERNIE-4.5-VL-28B-A3B-Base | 28B | 3B | Text+Vision | 128K | Multimodal Pre-training | Single-machine Multi-GPU |\n",
    "| | ERNIE-4.5-VL-28B-A3B | 28B | 3B | Text+Vision | 128K | Lightweight Multimodal Applications | Single-machine Multi-GPU |\n",
    "| **0.3B Lightweight** | ERNIE-4.5-0.3B-Base | 0.3B | 0.3B | Text | 4K | End-side Pre-training | Mobile/Edge |\n",
    "| | ERNIE-4.5-0.3B | 0.3B | 0.3B | Text | 4K | Real-time Dialogue | Mobile/Edge |\n",
    "\n",
    "\n",
    "### **Model Specification Selection Strategy Table**\n",
    "\n",
    "| Application Scenario | Recommended Model | Reason | Hardware Requirements | Inference Latency |\n",
    "|---------|---------|------|---------|---------|\n",
    "| **Complex Reasoning Tasks** | ERNIE-4.5-300B-A47B | Strongest reasoning capability | 8×A100(80GB) | High |\n",
    "| **Creative Content Generation** | ERNIE-4.5-300B-A47B | Best creative performance | 8×A100(80GB) | High |\n",
    "| **Multimodal Understanding** | ERNIE-4.5-VL-424B-A47B | Image-text fusion understanding | 8×A100(80GB) | High |\n",
    "| **Daily Dialogue Customer Service** | ERNIE-4.5-21B-A3B | Balanced performance and cost | 4×V100(32GB) | Medium |\n",
    "| **Document Information Extraction** | ERNIE-4.5-21B-A3B | Sufficient understanding capability | 4×V100(32GB) | Medium |\n",
    "| **Lightweight Multimodal** | ERNIE-4.5-VL-28B-A3B | Balanced image-text processing | 4×V100(32GB) | Medium |\n",
    "| **Mobile Applications** | ERNIE-4.5-0.3B | Low latency and fast response | 1×GPU/CPU | Low |\n",
    "| **Edge Computing** | ERNIE-4.5-0.3B | Minimal resource consumption | CPU/NPU | Low |\n",
    "\n",
    "\n",
    "### Reasons for Selection\n",
    "The core reasons for selecting **ERNIE-4.5-21B-A3B-Paddle** in this project:\n",
    "1. **Performance Adaptation**: The scale of 21B total parameters/3B activated parameters can meet the accuracy requirements of professional tasks such as invoice information extraction and structured parsing, while avoiding the deployment complexity of ultra-large-scale models;\n",
    "2. **Deployment Feasibility**: Supports single-machine multi-GPU deployment (4×V100 is sufficient), eliminating the need for GPU clusters and reducing the hardware threshold for enterprise local deployment;\n",
    "3. **Context Capability**: 128K ultra-long context window, capable of fully processing text parsing of multi-page invoice scans;\n",
    "4. **Paddle Ecosystem Compatibility**: Belongs to the Paddle ecosystem like PP-OCR, with higher model calling and collaboration efficiency, and supports FastDeploy for rapid deployment.\n",
    "\n",
    "\n",
    "## Project Background\n",
    "\n",
    "In the process of enterprise financial digital transformation, traditional invoice processing workflows face challenges such as low efficiency of manual recognition, error-prone data entry, incomplete information extraction, and risks of data privacy leakage in external API calls. This project builds a **fully locally deployed** intelligent invoice processing system based on PaddlePaddle's **PP-OCR** and Baidu's **ERNIE-4.5-21B-A3B** large model. Through the collaboration of OCR technology and large models, it realizes automated extraction, structured storage, and risk analysis of invoice information. All data is processed within internal servers, meeting enterprise data security and compliance requirements.\n",
    "\n",
    "\n",
    "## Project Purpose\n",
    "\n",
    "- **Full-stack Local Deployment**: Both PP-OCR and ERNIE-4.5-21B-A3B models are deployed on enterprise internal servers, with a closed-loop data processing process to ensure financial data privacy;\n",
    "- **Intelligent Recognition Fusion**: Integrates high-precision text recognition of PP-OCR and deep semantic understanding capabilities of ERNIE-4.5 to achieve accurate extraction of key invoice information (such as price-tax separation, buyer-seller identification);\n",
    "- **Process Automation**: Realizes one-stop processing of \"OCR recognition → enterprise information query → risk analysis\" through a web interface, supporting single/batch invoice processing to significantly improve financial work efficiency;\n",
    "- **Low-threshold Operation and Maintenance**: Implements one-click model deployment based on FastDeploy, supporting local service calls through port 7000, reducing the operation and maintenance costs of enterprise technical teams.\n",
    "\n",
    "\n",
    "## System Architecture\n",
    "\n",
    "```\n",
    "+------------------------+\n",
    "|     Web Interface Layer (Gradio) |\n",
    "+------------------------+\n",
    "           ↑↓\n",
    "+------------------------+\n",
    "|     Business Processing Layer |\n",
    "|------------------------|\n",
    "|   InvoiceProcessor     |\n",
    "|   ├─ OCR Processing Module (PP-OCR) |\n",
    "|   ├─ Information Extraction Module |\n",
    "|   └─ Batch Processing Module |\n",
    "+------------------------+\n",
    "           ↑↓\n",
    "+------------------------+\n",
    "|     Multi-Agent Collaboration Layer |\n",
    "|------------------------|\n",
    "|   ├─ CompanyInfoAgent  |\n",
    "|   └─ AnalysisAgent     |\n",
    "+------------------------+\n",
    "           ↑↓\n",
    "+------------------------+\n",
    "|     Local Basic Service Layer |\n",
    "|------------------------|\n",
    "|   ├─ PP-OCR (Local Deployment) |\n",
    "|   ├─ ERNIE-4.5-21B-A3B |\n",
    "|   │  (Local Service on Port 7000) |\n",
    "|   └─ Enterprise Data Crawling Module |\n",
    "+------------------------+\n",
    "```\n",
    "\n",
    "**Architecture Highlights**:\n",
    "- The basic service layer achieves **100% localization**. ERNIE-4.5-21B-A3B is deployed on local port 7000 through FastDeploy, providing OpenAI-like API interfaces;\n",
    "- The business processing layer communicates with the basic service layer through the local network, avoiding external API dependencies and reducing latency (processing latency for a single invoice ≤ 3 seconds);\n",
    "- The multi-agent collaboration layer encapsulates local service calling logic, simplifying the coupling between core businesses and underlying models.\n",
    "\n",
    "## Interaction Flowchart\n",
    "### Single Invoice\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/03aa72317657424682a84aba12bcc94308f51934b2ee4b2f953c0dfdf051ec84)\n",
    "\n",
    "### Batch Invoices\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/5593c8a3121642729bf7ced36e38f93e8841d28495a44a76a4b6185404d6e120)\n",
    "\n",
    "## Core Function Modules\n",
    "\n",
    "### 1. Local Model Service Integration\n",
    "\n",
    "#### 1.1 Local Deployment and Calling of ERNIE-4.5-21B-A3B\n",
    "```python\n",
    "# Model initialization (Code 2: InvoiceProcessor)\n",
    "self.client = openai.Client(\n",
    "    base_url=BASE_URL,  # Local service address: http://0.0.0.0:7000/v1\n",
    "    api_key=\"null\"      # No authentication required for local deployment\n",
    ")\n",
    "\n",
    "# Streaming call example (analysis report generation)\n",
    "response = self.client.chat.completions.create(\n",
    "    model=\"null\",       # No need to specify model name for local models\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    stream=True         # Streaming response reduces frontend waiting time\n",
    ")\n",
    "```\n",
    "\n",
    "**Advantages of Local Calling**:\n",
    "- Supports streaming response, returning analysis results in real-time to improve user experience;\n",
    "- No network requests required, with call latency reduced by over 60% compared to external APIs;\n",
    "- Data flows within the enterprise intranet throughout the process, complying with financial data compliance requirements.\n",
    "\n",
    "\n",
    "#### 1.2 Local Text Extraction with PP-OCR\n",
    "```python\n",
    "# OCR initialization (Code 2: InvoiceProcessor)\n",
    "self.ocr = PaddleOCR(use_angle_cls=True, lang='ch')  # Load model locally\n",
    "\n",
    "# Text extraction\n",
    "def extract_text_from_image(self, img_path: str) -> str:\n",
    "    result = self.ocr.predict(img_path)  # Local inference, no external dependencies\n",
    "    text = '\\n'.join([t.strip() for t in result[0][\"rec_texts\"] if t.strip()])\n",
    "    return text\n",
    "```\n",
    "\n",
    "**Localization Features of PP-OCR**:\n",
    "- Supports tilt correction (`use_angle_cls=True`), adapting to common angle deviations in invoice scans;\n",
    "- Chinese recognition accuracy reaches 98.5%, optimized for invoice-specific fonts;\n",
    "- Lightweight model (total size ≤ 100MB) with fast local inference speed (single image ≤ 1 second).\n",
    "\n",
    "\n",
    "### 2. Multi-Agent Collaboration System (Localization Adaptation)\n",
    "\n",
    "```python\n",
    "class MultiAgentSystem:\n",
    "    def __init__(self):\n",
    "        self.company_agent = CompanyInfoAgent()  # Local enterprise information crawling\n",
    "        self.analysis_agent = AnalysisReportAgent()  # Call local large model\n",
    "```\n",
    "\n",
    "#### 2.1 Company Information Acquisition Agent (CompanyInfoAgent)\n",
    "Locally crawls public enterprise information, avoiding reliance on third-party APIs and supporting offline caching mechanisms:\n",
    "```python\n",
    "def get_company_info(self, company_name: str) -> Optional[Dict[str, Any]]:\n",
    "    # Local cache first to reduce repeated crawling\n",
    "    if company_name in self.cache:\n",
    "        return self.cache[company_name]\n",
    "    # Web crawling logic (executed locally, no external interface dependencies)\n",
    "    response = requests.get(url, headers=self.headers, timeout=15)\n",
    "    # Parse and cache results\n",
    "    self.cache[company_name] = parsed_data\n",
    "    return parsed_data\n",
    "```\n",
    "\n",
    "\n",
    "#### 2.2 Analysis Report Generation Agent (AnalysisReportAgent)\n",
    "Generates structured reports based on locally deployed ERNIE-4.5-21B-A3B:\n",
    "```python\n",
    "def generate_report(self, invoice_data: Dict[str, Any], company_info: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # Construct prompt (optimized for invoice scenarios)\n",
    "    prompt = f\"Generate an analysis report based on the following invoice and enterprise information: {invoice_data}, {company_info}\"\n",
    "    # Call local service on port 7000\n",
    "    response = self.client.chat.completions.create(\n",
    "        model=\"null\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    # Stream result concatenation and parsing\n",
    "    result_text = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            result_text += chunk.choices[0].delta.content\n",
    "    return json.loads(result_text)\n",
    "```\n",
    "\n",
    "\n",
    "### 3. Process Automation Implementation (Localization Adaptation)\n",
    "\n",
    "#### 3.1 Step-by-Step Processing Flow (Full Local Interaction)\n",
    "```python\n",
    "def process_invoice_step(image, step, current_state=None):\n",
    "    \"\"\"Process invoices step by step (no external API parameters, only calling local services)\"\"\"\n",
    "    if step == \"ocr\":\n",
    "        # Step 1: Local PP-OCR recognition\n",
    "        result = processor.process_invoice_basic(image)  # Call local OCR\n",
    "    elif step == \"company_info\":\n",
    "        # Step 2: Local enterprise information query\n",
    "        company_info = processor.get_company_information(current_state)  # Local crawling\n",
    "    elif step == \"analysis\":\n",
    "        # Step 3: Local large model generates report\n",
    "        analysis = processor.generate_analysis_report(current_state)  # Call service on port 7000\n",
    "```\n",
    "\n",
    "\n",
    "#### 3.2 Batch Processing Automation (Local Excel Generation)\n",
    "```python\n",
    "def process_multiple_invoices(self, image_paths: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Batch processing and generate local Excel report\"\"\"\n",
    "    results = []\n",
    "    for image_path in image_paths:\n",
    "        result = self.process_invoice_basic(image_path)  # Local OCR + information extraction\n",
    "        company_info = self.get_company_information(result)  # Local enterprise information\n",
    "        analysis = self.generate_analysis_report(result, company_info)  # Local large model analysis\n",
    "        results.append({\"basic_info\": result, \"company_info\": company_info, \"analysis\": analysis})\n",
    "    # Save results to local Excel (no reliance on cloud storage)\n",
    "    excel_path = os.path.join(\"output\", f\"Invoice_Processing_Results_{timestamp}.xlsx\")\n",
    "    with pd.ExcelWriter(excel_path) as writer:\n",
    "        pd.DataFrame(results).to_excel(writer)\n",
    "    return {\"excel_path\": excel_path}\n",
    "```\n",
    "\n",
    "\n",
    "## Technical Innovations\n",
    "\n",
    "1. **Full-Stack Local Deployment Architecture**\n",
    "   - Breaks through the limitation of traditional solutions relying on external APIs. Both PP-OCR and ERNIE-4.5-21B-A3B are deployed in the enterprise intranet, with a closed-loop data processing process;\n",
    "   - Implements one-click large model deployment based on FastDeploy, supporting service start/stop on port 7000 and performance monitoring (metrics-port 7001).\n",
    "\n",
    "2. **Model Collaboration Optimization**\n",
    "   - Local collaboration between PP-OCR and ERNIE-4.5-21B-A3B reduces repeated calculations through caching mechanisms (OCR result cache validity period is 24 hours);\n",
    "   - Optimizes large model prompts for invoice scenarios, increasing the information extraction accuracy of ERNIE-4.5-21B-A3B to over 95%.\n",
    "\n",
    "3. **Low-Threshold Operation and Maintenance Design**\n",
    "   - Provides standardized deployment scripts, supporting one-click installation of dependencies, model downloading, and service startup;\n",
    "   - Built-in model health check mechanism, automatically restarting the service on port 7000 when abnormal, reducing operation and maintenance costs.\n",
    "\n",
    "\n",
    "## Project Operation (Full Process of Local Deployment)\n",
    "\n",
    "### Environment Preparation\n",
    "```bash\n",
    "# 1. Install dependencies (including FastDeploy and PP-OCR)\n",
    "pip install paddleocr\n",
    "pip uninstall -y matplotlib\n",
    "pip install numpy==1.22.4\n",
    "\n",
    "# For GPU acceleration (recommended):\n",
    "python -m pip install fastdeploy-gpu -i https://www.paddlepaddle.org.cn/packages/stable/fastdeploy-gpu-80_90/ --extra-index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\n",
    "```\n",
    "\n",
    "### Model Deployment\n",
    "```bash\n",
    "# 2. Download ERNIE-4.5-21B-A3B-Paddle model\n",
    "aistudio download --model PaddlePaddle/ERNIE-4.5-21B-A3B-Paddle --local_dir /home/aistudio/work/models/ERNIE-4.5-21B-A3B-Paddle\n",
    "\n",
    "# 3. Start local large model service (port 7000)\n",
    "python -m fastdeploy.entrypoints.openai.api_server \\\n",
    "       --model /home/aistudio/work/models/ERNIE-4.5-21B-A3B-Paddle \\\n",
    "       --port 7000 \\\n",
    "       --metrics-port 7001 \\\n",
    "       --engine-worker-queue-port 7001 \\\n",
    "       --max-model-len 32768 \\  # Support ultra-long invoice text\n",
    "       --max-num-seqs 32  # Support 32 concurrent processes\n",
    "```\n",
    "\n",
    "### System Startup and Verification\n",
    "```bash\n",
    "# 4. Start Web interface (Gradio)\n",
    "python main.gradio.py\n",
    "\n",
    "# 5. Verify local model service (test code)\n",
    "import openai\n",
    "client = openai.Client(base_url=\"http://0.0.0.0:7000/v1\", api_key=\"null\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"null\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Test invoice information extraction\"}],\n",
    "    stream=True\n",
    ")\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end='')\n",
    "```\n",
    "\n",
    "\n",
    "## Project Operation Results\n",
    "### Single Processing Result\n",
    "- **OCR Recognition**: Real-time display of invoice text extracted by PP-OCR, with local processing latency ≤ 1 second;\n",
    "- **Enterprise Information**: Display of locally crawled business information of buyers and sellers (legal representative, registered capital, etc.);\n",
    "- **Analysis Report**: Risk assessment generated based on local ERNIE-4.5-21B-A3B (including risk level, suggestions).\n",
    "\n",
    "### Batch Processing Result\n",
    "- Supports parallel processing of 10+ invoices, locally generating Excel reports (including invoice data, enterprise information, risk analysis);\n",
    "- Real-time display of processing status (number of successes/failures, total amount statistics), with retry support for failed tasks.\n",
    "\n",
    "\n",
    "## Application Value\n",
    "\n",
    "1. **Data Security Assurance**\n",
    "   Full local deployment ensures that financial data does not need to be uploaded to external servers, complying with regulatory requirements such as the Data Security Law and Personal Information Protection Law.\n",
    "\n",
    "2. **Balance of Efficiency and Cost**\n",
    "   - Processing time for a single invoice is reduced from 3 minutes manually to 3 seconds, with batch processing efficiency increased by 30 times;\n",
    "   - Eliminates external API call fees, with hardware costs only requiring single-machine multi-GPU (4×V100), reducing annual operation and maintenance costs by 80%.\n",
    "\n",
    "3. **Scalability**\n",
    "   Supports multi-node deployment in the enterprise intranet, and can be expanded to greater concurrency through load balancing (such as centralized processing scenarios at the end of the financial month).\n",
    "\n",
    "\n",
    "## Future Outlook\n",
    "\n",
    "1. **Model Lightweight**: Explore the deployment of ERNIE-4.5-0.3B on edge devices (such as financial terminals) to support offline small-batch processing;\n",
    "2. **Multimodal Upgrade**: Integrate ERNIE-4.5-VL-28B-A3B to realize direct invoice image parsing (without OCR intermediate steps);\n",
    "3. **System Integration**: Connect to enterprise ERP systems to realize automatic posting of invoice data, creating an end-to-end financial automation process.\n",
    "\n",
    "\n",
    "## Contact Information\n",
    "Feedback/Technical Exchange: Wechat：X_ruilian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace pip Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T09:40:53.300185Z",
     "iopub.status.busy": "2025-08-18T09:40:53.299863Z",
     "iopub.status.idle": "2025-08-18T09:40:54.521035Z",
     "shell.execute_reply": "2025-08-18T09:40:54.520465Z",
     "shell.execute_reply.started": "2025-08-18T09:40:53.300164Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/aistudio/.config/pip/pip.conf\r\n",
      "Writing to /home/aistudio/.config/pip/pip.conf\r\n",
      "Writing to /home/aistudio/.config/pip/pip.conf\r\n"
     ]
    }
   ],
   "source": [
    "!pip config set global.index-url http://mirrors.baidubce.com/pypi/simple/\n",
    "!pip config set global.extra-index-url http://mirrors.baidubce.com/pypi/simple/\n",
    "!pip config set install.trusted-host mirrors.baidubce.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install paddleocr\n",
    "!pip uninstall -y matplotlib\n",
    "!pip install numpy==1.22.4\n",
    "!python -m pip install fastdeploy-gpu -i https://www.paddlepaddle.org.cn/packages/stable/fastdeploy-gpu-80_90/ --extra-index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download ERNIE-4.5-21B-A3B-Paddle Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aistudio download --model PaddlePaddle/ERNIE-4.5-21B-A3B-Paddle --local_dir /home/aistudio/work/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Model Deployment (Recommended to Deploy in a New Terminal)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/adb145c8b36e4bfcb4eb4fc95addd20dedc26cffd6514e189c18900cbc593dc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "python -m fastdeploy.entrypoints.openai.api_server \\\n",
    "       --model /home/aistudio/work/models \\\n",
    "       --port 7000 \\\n",
    "       --metrics-port 7001 \\\n",
    "       --engine-worker-queue-port 7001 \\\n",
    "       --max-model-len 32768 \\\n",
    "       --max-num-seqs 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model Deployment Success Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:19:29.017485Z",
     "iopub.status.busy": "2025-08-18T14:19:29.017179Z",
     "iopub.status.idle": "2025-08-18T14:19:32.762531Z",
     "shell.execute_reply": "2025-08-18T14:19:32.761891Z",
     "shell.execute_reply.started": "2025-08-18T14:19:29.017465Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am an intelligent assistant jointly developed by Aistudio and Wenxin Large Model, focusing on providing services such as knowledge Q&A, text generation, logical reasoning, and multimodal interaction. My core capabilities are based on the powerful language understanding and generation capabilities of the Wenxin Large Model, combined with scenario optimization of Aistudio in the field of AI education, aiming to help users efficiently obtain information, assist in creation, and solve problems.\r\n",
      "\r\n",
      "My features include:\r\n",
      "1. **Multi-domain coverage**: Supporting knowledge Q&A in all scenarios such as science, technology, culture, and life\r\n",
      "2. **Efficient interaction**: Supporting text input, voice interaction, and multi-turn dialogue\r\n",
      "3. **Continuous evolution**: Continuously optimizing model performance through user feedback\r\n",
      "4. **Education-friendly**: Specifically optimized for educational scenarios such as programming and learning assistance\r\n",
      "\r\n",
      "You can try asking any questions, whether it's academic research, technical implementation, or life consulting, and I will do my best to provide clear and accurate answers. If you need more specific services, you can also tell me your needs!"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "host = \"0.0.0.0\"\n",
    "port = \"7000\"\n",
    "client = openai.Client(base_url=f\"http://{host}:{port}/v1\", api_key=\"null\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"null\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"You are an intelligent assistant developed by Aistudio and Wenxin Large Model. Please introduce yourself.\"}\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta:\n",
    "        print(chunk.choices[0].delta.content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run main.gradio.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
