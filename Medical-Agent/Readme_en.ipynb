{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【ERNIE-4.5-VL-28B】Multimodal Intelligent Medical Consultation System Based on ERNIE-4.5-VL-28B-Paddle Local Deployment + RAG + Multi-Agent Collaboration\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project builds an intelligent medical consultation system integrating RAG knowledge base retrieval and multi-Agent collaboration mechanisms based on the **locally deployed ERNIE-4.5-VL-28B-A3B-Paddle** multimodal large model. Through efficient local deployment of the model via the FastDeploy framework, combined with ChromaDB knowledge base and multimodal understanding capabilities, it provides users with professional medical consulting services.\n",
    "\n",
    "### 🎯 Project Highlights\n",
    "\n",
    "- **🏥 Complete Medical Scenarios**: Full-process intelligent consultation from symptom description to treatment recommendations\n",
    "- **🖼️ Multimodal Fusion**: Supports mixed input of text + images, capable of analyzing medical images such as skin lesions  \n",
    "- **🧠 Local Deployment**: Fully localized solution based on ERNIE-4.5-VL-28B-A3B-Paddle, ensuring data security and controllability\n",
    "- **📚 Knowledge Base Driven**: Medical knowledge base built with ChromaDB, supporting intelligent retrieval of symptoms, diseases, and treatment plans\n",
    "- **🤖 Multi-Agent Collaboration**: Collaborative work of professional Agents for symptom parsing, knowledge retrieval, diagnostic decision-making, etc.\n",
    "- **⚡ High-Performance Inference**: FastDeploy acceleration framework, single-machine multi-card deployment, and optimized inference latency\n",
    "\n",
    "### 🏗️ System Architecture Diagram\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[User Consultation Input] --> B[Gradio Frontend Interface]\n",
    "    B --> C[MedicalConsultation]\n",
    "    C --> D[AgentCoordinator]\n",
    "    \n",
    "    D --> E[ERNIE-4.5-VL Local Model]\n",
    "    E --> F[FastDeploy Inference Engine]\n",
    "    F --> G[Multimodal Understanding]\n",
    "    \n",
    "    D --> H[SymptomParserAgent]\n",
    "    D --> I[KnowledgeRetrievalAgent] \n",
    "    D --> J[DiagnosisAgent]\n",
    "    \n",
    "    I --> K[ChromaDB Knowledge Base]\n",
    "    K --> L[Symptom Database]\n",
    "    K --> M[Disease Database]\n",
    "    K --> N[Treatment Database]\n",
    "    \n",
    "    H --> O[Symptom Extraction]\n",
    "    I --> P[Knowledge Retrieval]\n",
    "    J --> Q[Risk Assessment]\n",
    "    J --> R[Treatment Recommendations]\n",
    "    \n",
    "    O --> S[Diagnostic Result Integration]\n",
    "    P --> S\n",
    "    Q --> S\n",
    "    R --> S\n",
    "    \n",
    "    S --> T[Structured Medical Report]\n",
    "    T --> B\n",
    "```\n",
    "\n",
    "### 🏆 Technical Innovation Summary\n",
    "\n",
    "This project realizes a complete technical chain from **large model local deployment** to **intelligent medical applications**:\n",
    "\n",
    "1. **🔥 Core Breakthrough**: Efficient local deployment of the 28B-parameter ERNIE-4.5-VL multimodal large model\n",
    "2. **🧠 Intelligent Upgrade**: RAG knowledge base retrieval + multi-Agent collaborative medical expert system  \n",
    "3. **🛡️ Data Security**: Fully localized solution, ensuring zero leakage of patient privacy\n",
    "4. **⚡ Performance Optimization**: FastDeploy inference acceleration, achieving second-level response for medical consultations\n",
    "\n",
    "### 🛠️ Technology Stack Selection\n",
    "\n",
    "| Layer | Technical Component | Version | Function |\n",
    "|------|---------|------|------|\n",
    "| **AI Model Layer** | ERNIE-4.5-VL-28B-A3B-Paddle | 28B parameters | Multimodal understanding and generation |\n",
    "| **Inference Framework** | FastDeploy | Latest version | Model deployment and inference acceleration |\n",
    "| **Knowledge Base** | ChromaDB | 1.0.15 | Vector database and semantic retrieval |\n",
    "| **Web Framework** | Gradio | 5.35.0 | Interactive user interface |\n",
    "| **Agent Framework** | Self-developed multi-Agent system | - | Task coordination and business logic |\n",
    "| **Data Processing** | Pillow + NumPy | 10.2.0 + 1.24.3 | Image processing and numerical calculation |\n",
    "\n",
    "## 🏥 Implementation of Intelligent Medical Consultation System\n",
    "\n",
    "### Core Function Modules\n",
    "\n",
    "#### 1. Multimodal Input Processing\n",
    "```python\n",
    "class ErnieClient:\n",
    "    def medical_image_analysis(self, image_path: str) -> str:\n",
    "        \"\"\"Medical image analysis\"\"\"\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "        \n",
    "        messages = [{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                {\"type\": \"text\", \"text\": \"Please analyze this medical image and describe the visible symptom characteristics\"}\n",
    "            ]\n",
    "        }]\n",
    "        # Call local ERNIE-4.5-VL model\n",
    "        return self._call_local_model(messages)\n",
    "```\n",
    "\n",
    "#### 2. Knowledge Base Retrieval System\n",
    "```python\n",
    "class KnowledgeBase:\n",
    "    def __init__(self, persist_directory=\"medical_kb\"):\n",
    "        # Build medical knowledge base using ChromaDB\n",
    "        self.client = chromadb.PersistentClient(path=persist_directory)\n",
    "        \n",
    "        # Create professional medical collections\n",
    "        self.symptoms_collection = self.client.get_or_create_collection(\"symptoms\")\n",
    "        self.diseases_collection = self.client.get_or_create_collection(\"diseases\")  \n",
    "        self.treatments_collection = self.client.get_or_create_collection(\"treatments\")\n",
    "```\n",
    "\n",
    "#### 3. Multi-Agent Collaboration System\n",
    "```python\n",
    "class AgentCoordinator:\n",
    "    def process_consultation(self, text_input: str, image_path: str = None):\n",
    "        # 1. Image analysis (if provided)\n",
    "        image_analysis = self.ernie.medical_image_analysis(image_path) if image_path else None\n",
    "        \n",
    "        # 2. Symptom parsing Agent\n",
    "        symptoms = self.symptom_parser.parse_symptoms(text_input, image_analysis)\n",
    "        \n",
    "        # 3. Knowledge retrieval Agent  \n",
    "        medical_info = self.knowledge_retriever.retrieve_relevant_info(symptoms)\n",
    "        \n",
    "        # 4. Diagnostic decision Agent\n",
    "        risk_assessment = self.diagnosis_agent.analyze_risk_level(symptoms, medical_info)\n",
    "        treatment_plan = self.diagnosis_agent.generate_treatment_plan(symptoms, medical_info)\n",
    "        \n",
    "        return {\n",
    "            \"symptoms\": symptoms,\n",
    "            \"risk_assessment\": risk_assessment, \n",
    "            \"treatment_plan\": treatment_plan,\n",
    "            \"image_analysis\": image_analysis\n",
    "        }\n",
    "```\n",
    "\n",
    "### 🎯 Application Scenarios and Effects\n",
    "\n",
    "#### Typical Usage Process\n",
    "1. **User Input**: Describe symptoms + upload lesion images (optional)\n",
    "2. **Multimodal Analysis**: ERNIE-4.5-VL understands both text and images simultaneously\n",
    "3. **Symptom Extraction**: AI identifies key symptoms and medical terms\n",
    "4. **Knowledge Retrieval**: Retrieve relevant disease information from professional medical databases\n",
    "5. **Risk Assessment**: Evaluate the severity of the condition and the urgency of medical treatment\n",
    "6. **Treatment Recommendations**: Generate personalized suggestions for examinations, medications, and lifestyle\n",
    "\n",
    "#### System Advantages\n",
    "- **Data Security**: Fully local deployment, ensuring patient data remains on-site\n",
    "- **Professional Accuracy**: Based on a professional medical model with 28B parameters\n",
    "- **Fast Response**: Local inference without network latency\n",
    "- **Continuous Learning**: The knowledge base can be continuously expanded and updated\n",
    "\n",
    "## 🧠 Selection of ERNIE-4.5-VL-28B-A3B-Paddle Model\n",
    "\n",
    "### Why Choose ERNIE-4.5-VL-28B-A3B-Paddle?\n",
    "\n",
    "#### Selection of Wenxin Open-Source Models\n",
    "\n",
    "### ERNIE-4.5 Model Series Specification Comparison Table\n",
    "\n",
    "| Model Series | Model Name | Total Parameters | Activated Parameters | Modal Support | Context Length | Main Usage | Deployment Scenario |\n",
    "|---------|---------|--------|---------|---------|-----------|---------|---------|\n",
    "| **A47B Large-Scale** | ERNIE-4.5-300B-A47B-Base | 300B | 47B | Text | 128K | Pre-training base | Cloud GPU cluster |\n",
    "| | ERNIE-4.5-300B-A47B | 300B | 47B | Text | 128K | Instruction following/creative generation | Cloud GPU cluster |\n",
    "| | ERNIE-4.5-VL-424B-A47B-Base | 424B | 47B | Text+Vision | 128K | Multimodal pre-training | Cloud GPU cluster |\n",
    "| | ERNIE-4.5-VL-424B-A47B | 424B | 47B | Text+Vision | 128K | Image-text understanding/generation | Cloud GPU cluster |\n",
    "| **A3B Medium-Scale** | ERNIE-4.5-21B-A3B-Base | 21B | 3B | Text | 128K | Pre-training base | Single-machine multi-card |\n",
    "| | ERNIE-4.5-21B-A3B | 21B | 3B | Text | 128K | Dialogue/document processing | Single-machine multi-card |\n",
    "| | ERNIE-4.5-VL-28B-A3B-Base | 28B | 3B | Text+Vision | 128K | Multimodal pre-training | Single-machine multi-card |\n",
    "| | ERNIE-4.5-VL-28B-A3B | 28B | 3B | Text+Vision | 128K | Lightweight multimodal applications | Single-machine multi-card |\n",
    "| **0.3B Lightweight** | ERNIE-4.5-0.3B-Base | 0.3B | 0.3B | Text | 4K | End-side pre-training | Mobile/edge |\n",
    "| | ERNIE-4.5-0.3B | 0.3B | 0.3B | Text | 4K | Real-time dialogue | Mobile/edge |\n",
    "\n",
    "### Model Specification Selection Strategy Table\n",
    "\n",
    "| Application Scenario | Recommended Model | Reason | Hardware Requirements | Inference Latency |\n",
    "|---------|---------|------|---------|---------|\n",
    "| **Complex reasoning tasks** | ERNIE-4.5-300B-A47B | Strongest reasoning ability | 8×A100(80GB) | High |\n",
    "| **Creative content generation** | ERNIE-4.5-300B-A47B | Best creative performance | 8×A100(80GB) | High |\n",
    "| **Multimodal understanding** | ERNIE-4.5-VL-424B-A47B | Image-text fusion understanding | 8×A100(80GB) | High |\n",
    "| **Daily dialogue customer service** | ERNIE-4.5-21B-A3B | Balanced performance and cost | 4×V100(32GB) | Medium |\n",
    "| **Document information extraction** | ERNIE-4.5-21B-A3B | Sufficient understanding ability | 4×V100(32GB) | Medium |\n",
    "| **Lightweight multimodal** | ERNIE-4.5-VL-28B-A3B | Balanced image-text processing | 4×V100(32GB) | Medium |\n",
    "| **Mobile applications** | ERNIE-4.5-0.3B | Low latency and fast response | 1×GPU/CPU | Low |\n",
    "| **Edge computing** | ERNIE-4.5-0.3B | Minimal resource consumption | CPU/NPU | Low |\n",
    "\n",
    "### Why Choose ERNIE-4.5-VL-28B-A3B-Paddle Model?\n",
    "\n",
    "#### 1. **Optimal Balance Between Performance and Cost**\n",
    "- **Moderate parameter scale**: 28B total parameters, 3B activated parameters, ensuring reasoning ability while controlling computing costs\n",
    "- **Reasonable hardware requirements**: Supports single-machine multi-card deployment (4×V100 or 2×A100), reducing hardware threshold by 75% compared to A47B series\n",
    "- **Moderate inference latency**: 3-5 times faster response speed than large-scale models while ensuring output quality\n",
    "\n",
    "#### 2. **Outstanding Multimodal Capabilities**\n",
    "- **Text+vision fusion**: Natively supports image-text understanding without additional visual encoders\n",
    "- **Long context support**: 128K token context length, capable of processing long documents and multiple images\n",
    "- **Rich application scenarios**: Suitable for document analysis, image description, multimodal question answering, etc.\n",
    "\n",
    "#### 3. **Deployment Friendliness**\n",
    "- **AIStudio platform optimization**: Official in-depth adaptation, providing one-click download and deployment\n",
    "- **FastDeploy integration**: Complete inference acceleration and service support\n",
    "- **Open-source ecosystem**: PaddlePaddle ecosystem with complete documentation and active community\n",
    "\n",
    "#### 4. **Practical Application Value**\n",
    "- **Enterprise-grade availability**: Significantly improved understanding and generation quality compared to 0.3B models\n",
    "- **Controllable cost**: 70% lower deployment cost and 60% lower operating cost compared to A47B series\n",
    "- **Strong scalability**: Supports LoRA fine-tuning, which can be optimized for specific scenarios\n",
    "\n",
    "#### 5. **Technical Advancement**\n",
    "- **MoE architecture**: Mixture of Experts model with activated parameters only 1/9 of total parameters, high inference efficiency\n",
    "- **Multimodal alignment**: Deep fusion of visual and text features, with understanding ability close to GPT-4V\n",
    "- **Chinese optimization**: In-depth optimization for Chinese scenarios, excellent performance in Chinese multimodal tasks\n",
    "\n",
    "#### Selection Recommendations\n",
    "**Recommended scenarios**:\n",
    "- Multimodal AI application development for small and medium-sized enterprises\n",
    "- Multimodal experiments in educational and scientific research projects\n",
    "- AI product prototype verification for individual developers\n",
    "- Business systems requiring image-text understanding capabilities\n",
    "\n",
    "**Not recommended scenarios**:\n",
    "- Real-time systems with extremely high requirements for inference latency (choose 0.3B)\n",
    "- Scenarios with sufficient budget and pursuit of ultimate performance (choose A47B)\n",
    "- Pure text applications without multimodal needs (choose 21B-A3B)\n",
    "\n",
    "## 🚀 ERNIE-4.5-VL Model Local Deployment Solution\n",
    "\n",
    "### I. Environment Preparation\n",
    "### 1. Hardware Requirements\n",
    "- **GPU**: NVIDIA A100 80GB (supports single-card/multi-card, CUDA 11.8+ recommended)  \n",
    "- **Memory**: ≥60GB RAM  \n",
    "- **Storage**: ≥60GB (model is approximately 28GB, reserved space for logs/cache)  \n",
    "\n",
    "### 2. Software Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T13:00:11.836046Z",
     "iopub.status.busy": "2025-07-06T13:00:11.835593Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m pip install fastdeploy-gpu -i https://www.paddlepaddle.org.cn/packages/stable/fastdeploy-gpu-80_90/ --extra-index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Environment Verification\n",
    "#### ① Check CUDA Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T17:25:11.092125Z",
     "iopub.status.busy": "2025-07-07T17:25:11.091792Z",
     "iopub.status.idle": "2025-07-07T17:25:11.310890Z",
     "shell.execute_reply": "2025-07-07T17:25:11.310222Z",
     "shell.execute_reply.started": "2025-07-07T17:25:11.092105Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\r\n",
      "Built on Tue_Oct_29_23:50:19_PDT_2024\r\n",
      "Cuda compilation tools, release 12.6, V12.6.85\r\n",
      "Build cuda_12.6.r12.6/compiler.35059454_0\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "# Expected output example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ② Check GPU Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T17:25:27.824496Z",
     "iopub.status.busy": "2025-07-07T17:25:27.824188Z",
     "iopub.status.idle": "2025-07-07T17:25:28.730838Z",
     "shell.execute_reply": "2025-07-07T17:25:28.730260Z",
     "shell.execute_reply.started": "2025-07-07T17:25:27.824477Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  8 01:25:28 2025       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  NVIDIA A800-SXM4-80GB          On  |   00000000:D3:00.0 Off |                    0 |\r\n",
      "| N/A   41C    P0             66W /  400W |       0MiB /  81920MiB |      0%      Default |\r\n",
      "|                                         |                        |             Disabled |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Check GPU status and memory\n",
    "!nvidia-smi\n",
    "# Expected output example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ③ Troubleshooting Common Environment Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check PaddlePaddle GPU support\n",
    "!python -c \"import paddle; print('GPU available:', paddle.is_compiled_with_cuda()); print('Number of GPU devices:', paddle.device.cuda.device_count())\"\n",
    "\n",
    "# Check FastDeploy installation\n",
    "!python -c \"from fastdeploy import LLM, SamplingParams; print('FastDeploy installed successfully!')\"\n",
    "\n",
    "# Check OpenAI library version\n",
    "!python -c \"import openai; print('OpenAI library version:', openai.__version__)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Model Download and Directory Structure\n",
    "### 1. Download Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Download model using AIStudio command\n",
    "!aistudio download --model PaddlePaddle/ERNIE-4.5-VL-28B-A3B-Paddle --local_dir /home/aistudio/work/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T17:28:11.223932Z",
     "iopub.status.busy": "2025-07-07T17:28:11.223616Z",
     "iopub.status.idle": "2025-07-07T17:28:11.427076Z",
     "shell.execute_reply": "2025-07-07T17:28:11.426503Z",
     "shell.execute_reply.started": "2025-07-07T17:28:11.223912Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 57441133\r\n",
      "-rw-r--r-- 1 aistudio aistudio      11366 Jul  6 18:57 LICENSE\r\n",
      "-rw-r--r-- 1 aistudio aistudio       9077 Jul  6 18:56 README.md\r\n",
      "-rw-r--r-- 1 aistudio aistudio      86904 Jul  6 18:56 added_tokens.json\r\n",
      "-rw-r--r-- 1 aistudio aistudio       1306 Jul  6 18:57 config.json\r\n",
      "-rw-r--r-- 1 aistudio aistudio        134 Jul  6 18:57 generation_config.json\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4991326368 Jul  6 18:57 model-00001-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4988696384 Jul  6 18:56 model-00002-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4999185600 Jul  6 18:56 model-00003-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4995268296 Jul  6 18:57 model-00004-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4988696984 Jul  6 18:56 model-00005-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4999193256 Jul  6 18:57 model-00006-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4995261896 Jul  6 18:57 model-00007-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4999183248 Jul  6 18:56 model-00008-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4995271816 Jul  6 18:57 model-00009-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4988696992 Jul  6 18:57 model-00010-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4999190216 Jul  6 18:57 model-00011-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 3877260800 Jul  6 18:57 model-00012-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio     691887 Jul  6 18:56 model.safetensors.index.json\r\n",
      "-rw-r--r-- 1 aistudio aistudio        615 Jul  6 18:56 preprocessor_config.json\r\n",
      "-rw-r--r-- 1 aistudio aistudio      62694 Jul  6 18:56 special_tokens_map.json\r\n",
      "-rw-r--r-- 1 aistudio aistudio    1614362 Jul  6 18:56 tokenizer.model\r\n",
      "-rw-r--r-- 1 aistudio aistudio       3606 Jul  6 18:56 tokenizer_config.json\r\n"
     ]
    }
   ],
   "source": [
    "# View model files\n",
    "!ls -l work/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Directory Structure\n",
    "```\n",
    "work/\n",
    "└── models/\n",
    "    ├── LICENSE                           # License file\n",
    "    ├── README.md                         # Model description document\n",
    "    ├── added_tokens.json                 # Added token configuration\n",
    "    ├── config.json                       # Model configuration file\n",
    "    ├── generation_config.json            # Generation configuration file\n",
    "    ├── model-00001-of-00012.safetensors  # Model parameter file (shard 1/12)\n",
    "    ├── model-00002-of-00012.safetensors  # Model parameter file (shard 2/12)\n",
    "    ├── model-00003-of-00012.safetensors  # Model parameter file (shard 3/12)\n",
    "    ├── model-00004-of-00012.safetensors  # Model parameter file (shard 4/12)\n",
    "    ├── model-00005-of-00012.safetensors  # Model parameter file (shard 5/12)\n",
    "    ├── model-00006-of-00012.safetensors  # Model parameter file (shard 6/12)\n",
    "    ├── model-00007-of-00012.safetensors  # Model parameter file (shard 7/12)\n",
    "    ├── model-00008-of-00012.safetensors  # Model parameter file (shard 8/12)\n",
    "    ├── model-00009-of-00012.safetensors  # Model parameter file (shard 9/12)\n",
    "    ├── model-00010-of-00012.safetensors  # Model parameter file (shard 10/12)\n",
    "    ├── model-00011-of-00012.safetensors  # Model parameter file (shard 11/12)\n",
    "    ├── model-00012-of-00012.safetensors  # Model parameter file (shard 12/12)\n",
    "    ├── model.safetensors.index.json      # Model shard index file\n",
    "    ├── preprocessor_config.json          # Preprocessor configuration\n",
    "    ├── special_tokens_map.json           # Special token mapping\n",
    "    ├── tokenizer.model                   # Tokenizer model file\n",
    "    └── tokenizer_config.json             # Tokenizer configuration file\n",
    "```\n",
    "\n",
    "### 3. File Description\n",
    "| File Type | File Name | Description |\n",
    "|---------|--------|------|\n",
    "| **Model weights** | model-00001~00012-of-00012.safetensors | Model parameter shard files in safe tensor format |\n",
    "| **Index file** | model.safetensors.index.json | Model shard index, specifying which shard each parameter is in |\n",
    "| **Configuration file** | config.json | Model architecture configuration, including number of layers, hidden layer size, etc. |\n",
    "| **Generation configuration** | generation_config.json | Text generation-related configurations, such as maximum length, sampling parameters, etc. |\n",
    "| **Tokenizer** | tokenizer.model | SentencePiece tokenizer model |\n",
    "| **Tokenizer configuration** | tokenizer_config.json | Tokenizer configuration parameters |\n",
    "| **Preprocessor** | preprocessor_config.json | Image preprocessing configuration (for multimodal models only) |\n",
    "| **Special tokens** | special_tokens_map.json | Special token mapping, such as padding, unknown, etc. |\n",
    "| **Added tokens** | added_tokens.json | User-defined added tokens |\n",
    "\n",
    "## III. Start Service (Key Commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T10:57:33.976246Z",
     "iopub.status.busy": "2025-07-06T10:57:33.975924Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\r\n",
      "  warnings.warn(warning_message)\r\n",
      "/home/aistudio/external-libraries/lib/python3.10/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\r\n",
      "  warnings.warn(\r\n",
      "\u001b[32m[2025-07-06 18:57:55,035] [    INFO]\u001b[0m - loading configuration file work/models/preprocessor_config.json\u001b[0m\r\n",
      "INFO     2025-07-06 18:57:58,188 5574  engine.py[line:206] Waitting worker processes ready...\r\n",
      "Loading Weights: 100%|████████████████████████| 100/100 [01:17<00:00,  1.30it/s]\r\n",
      "Loading Layers: 100%|█████████████████████████| 100/100 [00:07<00:00, 13.31it/s]\r\n",
      "INFO     2025-07-06 18:59:35,857 5574  engine.py[line:276] Worker processes are launched with 119.6253821849823 seconds.\r\n",
      "INFO     2025-07-06 18:59:35,857 5574  api_server.py[line:91] Launching metrics service at http://0.0.0.0:8181/metrics\r\n",
      "INFO     2025-07-06 18:59:35,857 5574  api_server.py[line:94] Launching chat completion service at http://0.0.0.0:8180/v1/chat/completions\r\n",
      "INFO     2025-07-06 18:59:35,857 5574  api_server.py[line:97] Launching completion service at http://0.0.0.0:8180/v1/completions\r\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m5574\u001b[0m]\r\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\r\n",
      "\u001b[32m[2025-07-06 18:59:54,481] [    INFO]\u001b[0m - loading configuration file work/models/preprocessor_config.json\u001b[0m\r\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\r\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8180\u001b[0m (Press CTRL+C to quit)\r\n"
     ]
    }
   ],
   "source": [
    "python -m fastdeploy.entrypoints.openai.api_server \\\n",
    "       --model work/models \\\n",
    "       --port 8180 \\\n",
    "       --metrics-port 8181 \\\n",
    "       --engine-worker-queue-port 8182 \\\n",
    "       --max-model-len 32768 \\\n",
    "       --enable-mm \\\n",
    "       --reasoning-parser ernie-45-vl \\\n",
    "       --max-num-seqs 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Description of Service Startup Parameters\n",
    "\n",
    "| Parameter Name | Description | Default Value | Example Value |\n",
    "|---------|---------|-------|-------|\n",
    "| `--model` | Path to model files, directory containing model weights and configuration files | Required | `work/models` |\n",
    "| `--port` | API service listening port, through which clients access the service | 8000 | `8180` |\n",
    "| `--metrics-port` | Monitoring metrics service port, used for performance monitoring and health checks | 8001 | `8181` |\n",
    "| `--engine-worker-queue-port` | Engine worker queue port, used for internal task scheduling | 8002 | `8182` |\n",
    "| `--max-model-len` | Maximum sequence length (number of tokens) supported by the model | 2048 | `32768` |\n",
    "| `--enable-mm` | Enable multimodal function (text+image processing) | False | `Enabled` |\n",
    "| `--reasoning-parser` | Reasoning parser type, specifying the model's reasoning logic | None | `ernie-45-vl` |\n",
    "| `--max-num-seqs` | Maximum number of concurrent sequences, controlling batch size | 256 | `32` |\n",
    "\n",
    "### Other Common Parameters\n",
    "\n",
    "| Parameter Name | Description | Default Value | Remarks |\n",
    "|---------|---------|-------|------|\n",
    "| `--host` | Host address bound by the service | 0.0.0.0 | Set to 0.0.0.0 to allow external access |\n",
    "| `--trust-remote-code` | Trust remote code execution | False | Required when loading custom models |\n",
    "| `--tensor-parallel-size` | Tensor parallel size (multi-GPU) | 1 | Set according to the number of GPUs |\n",
    "| `--gpu-memory-utilization` | GPU memory utilization | 0.9 | Recommended between 0.8-0.95 |\n",
    "| `--max-num-batched-tokens` | Maximum number of batched tokens | Automatically calculated | Adjust according to GPU memory |\n",
    "| `--swap-space` | Swap space size (GB) | 4 | Used when memory is insufficient |\n",
    "| `--enable-lora` | Enable LoRA adapter | False | Used when fine-tuning models |\n",
    "| `--max-log-len` | Maximum log length | Unlimited | Control log file size |\n",
    "\n",
    "### Service Verification\n",
    "```bash\n",
    "# Check if the model is loaded correctly\n",
    "curl http://localhost:8180/v1/models\n",
    "\n",
    "# Expected output (contains model ID)\n",
    "{\"data\":[{\"id\":\"ernie-4.5-vl-28b-a3b-paddle\",\"object\":\"model\"}]}\n",
    "```\n",
    "\n",
    "## IV. Model Calling Examples\n",
    "\n",
    "### 1. OpenAI Library Calling Method\n",
    "\n",
    "#### ① Text Generation\n",
    "```python\n",
    "import openai\n",
    "\n",
    "host = \"0.0.0.0\"\n",
    "port = \"8180\"\n",
    "client = openai.Client(base_url=f\"http://{host}:{port}/v1\", api_key=\"null\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"null\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"You are an intelligent assistant developed by Aistudio and Wenxin large model. Please introduce yourself.\"}\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta:\n",
    "        print(chunk.choices[0].delta.content, end='')\n",
    "```\n",
    "\n",
    "#### ② Image Description Generation\n",
    "```python\n",
    "import openai\n",
    "import base64\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "client = openai.Client(\n",
    "    base_url=\"http://0.0.0.0:8180/v1\",\n",
    "    api_key=\"null\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"null\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image('1.jpg')}\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Generate a description of this image\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"Image description: \", end='', flush=True)\n",
    "for chunk in response:\n",
    "    if chunk.choices and chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end='', flush=True)\n",
    "print()\n",
    "```\n",
    "\n",
    "### 2. Requests Library Calling Method\n",
    "\n",
    "#### ① Text Generation\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:8180/v1/chat/completions\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "    \"model\": \"null\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"You are an intelligent assistant developed by Aistudio and Wenxin large model. Please introduce yourself.\"}\n",
    "    ],\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload, stream=True)\n",
    "\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        line = line.decode('utf-8').replace('data: ', '')\n",
    "        \n",
    "        if line.strip() == '[DONE]':\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            \n",
    "            if 'choices' in data and len(data['choices']) > 0:\n",
    "                delta = data['choices'][0].get('delta', {})\n",
    "                content = delta.get('content', '')\n",
    "                if content:\n",
    "                    print(content, end='', flush=True)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding line: {line}\")\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "print()\n",
    "```\n",
    "\n",
    "#### ② Image Description Generation\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "url = \"http://localhost:8180/v1/chat/completions\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "    \"model\": \"null\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image('1.jpg')}\"}},\n",
    "                {\"type\": \"text\", \"text\": \"Generate image description\"}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload, stream=True)\n",
    "\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        line = line.decode('utf-8').replace('data: ', '')\n",
    "        \n",
    "        if line.strip() == '[DONE]':\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            \n",
    "            if 'choices' in data and len(data['choices']) > 0:\n",
    "                delta = data['choices'][0].get('delta', {})\n",
    "                content = delta.get('content', '')\n",
    "                if content:\n",
    "                    print(content, end='', flush=True)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding line: {line}\")\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "print()\n",
    "```\n",
    "\n",
    "### 3. Description of Calling Parameters\n",
    "\n",
    "#### Common Parameter Configuration\n",
    "```python\n",
    "# Complete calling parameter example\n",
    "response = client.chat.completions.create(\n",
    "    model=\"null\",                    # Model name (fixed value)\n",
    "    messages=[...],                  # Message list\n",
    "    stream=True,                     # Whether to enable streaming response\n",
    "    max_tokens=2048,                 # Maximum number of generated tokens\n",
    "    temperature=0.7,                 # Temperature parameter (0.0-2.0)\n",
    "    top_p=0.9,                      # Nucleus sampling parameter (0.0-1.0)\n",
    "    frequency_penalty=0.0,           # Frequency penalty\n",
    "    presence_penalty=0.0,            # Presence penalty\n",
    "    stop=[\"<|endoftext|>\"],         # Stop word list\n",
    ")\n",
    "```\n",
    "\n",
    "#### Detailed Parameter Description\n",
    "| Parameter Name | Type | Default Value | Description |\n",
    "|---------|------|--------|------|\n",
    "| `model` | str | \"null\" | Model name, fixed as \"null\" for local deployment |\n",
    "| `messages` | list | Required | List of dialogue messages, including role and content |\n",
    "| `stream` | bool | False | Whether to enable streaming response |\n",
    "| `max_tokens` | int | Automatic | Maximum number of generated tokens |\n",
    "| `temperature` | float | 1.0 | Controls randomness, higher values are more random |\n",
    "| `top_p` | float | 1.0 | Nucleus sampling, controls the range of vocabulary selection |\n",
    "| `frequency_penalty` | float | 0.0 | Frequency penalty, reduces repetitive content |\n",
    "| `presence_penalty` | float | 0.0 | Presence penalty, encourages talking about new topics |\n",
    "| `stop` | list | None | List of strings to stop generation |\n",
    "\n",
    "## V. Common Problems and Solutions\n",
    "### 1. Port Occupation Problem\n",
    "```bash\n",
    "# Check process occupying the port\n",
    "lsof -i:8180\n",
    "\n",
    "# Terminate the process (replace <PID> with the actual process ID)\n",
    "kill -9 <PID>\n",
    "```\n",
    "\n",
    "### 2. Model Loading Failure\n",
    "- **Check directory**: Ensure complete model files (.pdparams/config.json/vocab.txt) exist under `work/models`  \n",
    "- **Parser parameters**: Ensure the startup command includes `--reasoning-parser ernie-45-vl`  \n",
    "- **Driver version**: NVIDIA driver must be ≥520.61.05 (supports A100)  \n",
    "\n",
    "### 3. Abnormal Streaming Response\n",
    "- Ensure the `stream=True` parameter is correctly passed  \n",
    "- Check service logs (`fastdeploy_server.log`) for insufficient memory/video memory errors  \n",
    "\n",
    "\n",
    "## VI. Test Cases\n",
    "### 1. Text Generation Verification\n",
    "- **Input**: `\"Summarize the core steps of this tutorial\"`  \n",
    "- **Expected**: Output coherent text containing keywords such as \"environment configuration\", \"service startup\", \"interface calling\", etc.  \n",
    "\n",
    "### 2. Image Description Verification\n",
    "- **Test image**: Use jpg files containing natural scenes (such as landscapes, people's activities)  \n",
    "- **Expected**: Output sentences containing scene features (such as \"green grassland under blue sky and white clouds\") and action descriptions (such as \"people walking by the lake\")  \n",
    "\n",
    "## VII. Intelligent Medical Consultation System Deployment\n",
    "\n",
    "### 1. System Dependency Installation\n",
    "```bash\n",
    "# Update dependency package versions\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### 2. Medical Knowledge Base Initialization\n",
    "```bash\n",
    "# Initialize ChromaDB medical knowledge base\n",
    "python init_knowledge_base.py\n",
    "\n",
    "# After startup, the following collections will be created:\n",
    "# - symptoms: Symptom knowledge base\n",
    "# - diseases: Disease knowledge base  \n",
    "# - treatments: Treatment plan base\n",
    "```\n",
    "\n",
    "### 3. Start the Complete System\n",
    "```bash\n",
    "# Step 1: Start ERNIE-4.5-VL model service\n",
    "python -m fastdeploy.entrypoints.openai.api_server \\\n",
    "       --model work/models \\\n",
    "       --port 8180 \\\n",
    "       --enable-mm \\\n",
    "       --reasoning-parser ernie-45-vl\n",
    "\n",
    "# Step 2: Start the medical consultation Web interface  \n",
    "python main.gradio.py\n",
    "```\n",
    "\n",
    "### 4. System Verification\n",
    "```bash\n",
    "# Run system tests\n",
    "python test_system.py\n",
    "\n",
    "# Expected output:\n",
    "# ✅ ERNIE service connection successful\n",
    "# ✅ Knowledge base connection normal  \n",
    "# ✅ Symptom analysis function normal\n",
    "# ✅ Consultation process completed\n",
    "```\n",
    "\n",
    "## VIII. Function Display and Effects\n",
    "\n",
    "### 🖼️ System Interface\n",
    "![Intelligent Medical Consultation System Main Interface](https://ai-studio-static-online.cdn.bcebos.com/9071eee410474644988213a51be33c75de6f65b21f5b44328d25094600a98361)\n",
    "\n",
    "### 📋 Diagnostic Report Example\n",
    "![Multimodal Consultation Result](https://ai-studio-static-online.cdn.bcebos.com/b899c223a17849f3ae0414b569f480bc24c98fc54fbf4923b57e24eb15850c65)\n",
    "\n",
    "### 🔍 Image Analysis Capability  \n",
    "![Skin Lesion Image Analysis](https://ai-studio-static-online.cdn.bcebos.com/be14a288a7a848a783b700d5292b767eddf11bee1253412c95460e0619dadba2)\n",
    "\n",
    "### 💊 Treatment Recommendation Output Format\n",
    "```\n",
    "【Symptom Analysis】\n",
    "Identified symptoms: fever, cough, fatigue\n",
    "\n",
    "【Risk Assessment】  \n",
    "Risk level: ⚠️⚠️ (Recommend ordinary outpatient visit)\n",
    "Recommendations:\n",
    "- Symptoms have persisted for a short time, no serious complications\n",
    "- It is recommended to seek medical attention promptly to rule out infectious diseases\n",
    "\n",
    "【Recommended Examination Items】\n",
    "- Routine blood test\n",
    "- C-reactive protein test\n",
    "- Chest X-ray\n",
    "\n",
    "【Medication Recommendations】\n",
    "- Symptomatic treatment: Ibuprofen for fever reduction\n",
    "- Cough relief and phlegm reduction: Compound Glycyrrhiza Tablets\n",
    "- Please take medication as directed by a doctor, avoid self-medication\n",
    "\n",
    "【Lifestyle Recommendations】  \n",
    "- Adequate rest, avoid strenuous exercise\n",
    "- Drink plenty of water, keep indoor ventilation\n",
    "- Keep warm, avoid catching cold again\n",
    "```\n",
    "\n",
    "## IX. Technical Innovation Points\n",
    "\n",
    "### 🔬 Core Technical Advantages\n",
    "\n",
    "#### 1. Localized Multimodal Large Model\n",
    "- **Model scale**: Efficient MoE architecture with 28B parameters and 3B activated parameters\n",
    "- **Multimodal capability**: Natively supports joint understanding of text + images\n",
    "- **Deployment optimization**: FastDeploy framework, efficient inference with single-machine multi-card\n",
    "- **Data security**: Fully localized, zero leakage of patient privacy\n",
    "\n",
    "#### 2. RAG-Enhanced Knowledge System\n",
    "- **Vectorized storage**: Efficient semantic retrieval built with ChromaDB\n",
    "- **Hierarchical knowledge base**: Structured management of symptoms, diseases, and treatment plans\n",
    "- **Real-time retrieval**: Millisecond-level similarity matching and knowledge recall\n",
    "- **Dynamic update**: Supports incremental update and expansion of the knowledge base\n",
    "\n",
    "#### 3. Multi-Agent Collaboration Architecture\n",
    "- **Modular design**: Each Agent is responsible for specific medical tasks\n",
    "- **Intelligent orchestration**: AgentCoordinator for unified scheduling and data flow management\n",
    "- **Fault-tolerance mechanism**: Failure of a single Agent does not affect the overall system operation\n",
    "- **Scalability**: New medical specialty Agents can be plug-and-play\n",
    "\n",
    "#### 4. User Experience Optimization\n",
    "- **Streaming response**: Real-time display of AI analysis process, improving interaction experience\n",
    "- **Multi-terminal adaptation**: Web interface supports PC and mobile access\n",
    "- **Result visualization**: Structured medical reports for easy understanding and saving\n",
    "- **Operation convenience**: Drag-and-drop image upload, quick text input\n",
    "\n",
    "## X. Application Value and Scenarios\n",
    "\n",
    "### 🏥 Medical Scenario Applications\n",
    "\n",
    "#### Primary Medical Institutions\n",
    "- **Preliminary consultation**: Assisting general practitioners in symptom analysis\n",
    "- **Triage assistance**: Assessing the urgency of patients' conditions  \n",
    "- **Knowledge support**: Providing doctors with reference for disease diagnosis and treatment\n",
    "\n",
    "#### Telemedicine Services\n",
    "- **Online consultation**: 24-hour intelligent medical consultation service\n",
    "- **Image diagnosis**: Analysis of visual diseases such as skin diseases and traumas\n",
    "- **Health education**: Providing professional health management advice\n",
    "\\n",
    "\n",
    "#### Personal Health Management\n",
    "- **Symptom self-check**: Users independently assess their health status\n",
    "- **Medical guidance**: Providing scientific medical advice and department recommendations\n",
    "- **Medication consultation**: Safe medication guidance based on symptoms\n",
    "\n",
    "### 💡 Technical Value\n",
    "\n",
    "#### Industry Promotion\n",
    "- **AI medical standardization**: Technical paradigm for multimodal medical AI\n",
    "- **Local deployment**: Providing solutions for medical data security\n",
    "- **Open-source ecosystem**: Complete technology stack based on PaddlePaddle\n",
    "\n",
    "#### Innovation Breakthroughs\n",
    "- **Multimodal fusion**: Integrated medical understanding capability of images and texts\n",
    "- **Knowledge base driven**: In-depth application of RAG technology in the medical field\n",
    "- **Agent collaboration**: Collaboration mechanism for professional AI systems\n",
    "\n",
    "## XI. System Monitoring and Maintenance\n",
    "\n",
    "### 📊 Performance Monitoring\n",
    "```bash\n",
    "# Check model service status\n",
    "curl http://localhost:8180/v1/models\n",
    "\n",
    "# Monitor system resource usage\n",
    "nvidia-smi  # GPU usage\n",
    "top         # CPU and memory usage\n",
    "\n",
    "# View service logs\n",
    "tail -f logs/gradio_app_*.log\n",
    "\n",
    "\n",
    "### 📞 Contact Information\n",
    "- **Project Author**: Wechat: X_ruilian"
   ]
  },
 {
    "cell_type": "code",
    "execution_count": 27,
    "metadata": {
      "execution": {
        "iopub.execute_input": "2025-07-06T13:47:22.113188Z",
        "iopub.status.busy": "2025-07-06T13:47:22.112841Z",
        "iopub.status.idle": "2025-07-06T13:47:36.519914Z",
        "shell.execute_reply": "2025-07-06T13:47:36.519349Z",
        "shell.execute_reply.started": "2025-07-06T13:47:22.113166Z"
      },
      "scrolled": true,
      "tags": []
    },
    "outputs": [
      {
        "name": "stderr",
        "output_type": "stream",
        "text": [
          "2025-07-06 21:47:22,130 - INFO - HTTP Request: POST http://0.0.0.0:8180/v1/chat/completions \"HTTP/1.1 200 OK\"\r\n"
        ]
      },
      {
        "name": "stdout",
        "output_type": "stream",
        "text": [
          "\r\n",
          "\r\n",
          "I am an intelligent assistant empowered by **Aistudio** (Baidu's open-source AI development platform) and the **Wenxin Large Model** technology framework. Here is an introduction to my core features and functions:\r\n",
          "\r\n",
          "### 1. **Technical Background**\r\n",
          "   - Based on the foundational capabilities of Baidu's self-developed **Wenxin Large Model**, integrating multimodal understanding and generation technologies, with strong natural language processing capabilities.\r\n",
          "   - Relying on Aistudio's open-source ecosystem to support developer collaboration and model iteration optimization.\r\n",
          "\r\n",
          "### 2. **Core Capabilities**\r\n",
          "   - **Knowledge Q&A**: Covering a wide range of fields (technology, culture, daily life, etc.), providing accurate and concise answers.\r\n",
          "   - **Text Generation**: Capable of writing articles, code, poems, dialogues, etc., supporting both creative and practical scenarios.\r\n",
          "   - **Logical Reasoning**: Analyzing complex problems and providing structured thinking paths.\r\n",
          "   - **Multilingual Support**: Primarily focused on Chinese, with support for interactions in English and other languages.\r\n",
          "\r\n",
          "### 3. **Application Scenarios**\r\n",
          "   - **Learning Assistance**: Answering subject-related questions, generating study notes, or simulating dialogues.\r\n",
          "   - **Content Creation**: Assisting in writing, script generation, marketing copy planning, etc.\r\n",
          "   - **Daily Assistant**: Providing life advice, time planning, and interesting interactions.\r\n",
          "   - **Technical Collaboration**: Connecting with developers to optimize code logic or model training plans.\r\n",
          "\r\n",
          "### 4. **Advantages**\r\n",
          "   - **Continuous Learning**: Maintaining knowledge timeliness through user feedback and data updates.\r\n",
          "   - **Safe and Controllable**: Adhering to ethical norms to avoid generating harmful content.\r\n",
          "   - **Flexible Interaction**: Supporting free dialogue or specified tasks (e.g., \"Write a poem about spring\").\r\n",
          "\r\n",
          "### 5. **How to Use**\r\n",
          "   - Directly input needs through dialogue or specify task goals (e.g., \"Generate Python crawler code\").\r\n",
          "   - Combined with the Aistudio platform, you can participate in model training, deployment, or open-source project collaboration.\r\n",
          "\r\n",
          "I look forward to providing efficient support for your study, work, or life through my capabilities! 😊"
        ]
      }
    ],
    "source": [
      "import openai\n",
      "host = \"0.0.0.0\"\n",
      "port = \"8180\"\n",
      "client = openai.Client(base_url=f\"http://{host}:{port}/v1\", api_key=\"null\")\n",
      "\n",
      "response = client.chat.completions.create(\n",
      "    model=\"null\",\n",
      "    messages=[\n",
      "        {\"role\": \"user\", \"content\": \"你是Aistudio和文心大模型开发的智能助手，请介绍一下你自己.\"}\n",
      "    ],\n",
      "    stream=True,\n",
      ")\n",
      "for chunk in response:\n",
      "    if chunk.choices[0].delta:\n",
      "        print(chunk.choices[0].delta.content, end='')"
    ]
  },
  {
    "cell_type": "code",
    "execution_count": 4,
    "metadata": {
      "execution": {
        "iopub.execute_input": "2025-07-06T12:17:04.177492Z",
        "iopub.status.busy": "2025-07-06T12:17:04.177129Z",
        "iopub.status.idle": "2025-07-06T12:17:19.079569Z",
        "shell.execute_reply": "2025-07-06T12:17:19.078882Z",
        "shell.execute_reply.started": "2025-07-06T12:17:04.177471Z"
      },
      "scrolled": true,
      "tags": []
    },
    "outputs": [
      {
        "name": "stdout",
        "output_type": "stream",
        "text": [
          "Image description:\r\n",
          "This image shows the core architecture and functional modules of PaddleNLP (PaddlePaddle Natural Language Processing development library). The overall design is concise and clear, using blue and white colors to highlight a sense of technology. The image is divided into three main parts:\r\n",
          "\r\n",
          "### 1. **Industrial-grade Predefined Tasks (Taskflow)**\r\n",
          "   - **Natural Language Understanding**: Including lexical analysis, text error correction, sentiment analysis, and syntactic analysis, covering basic text processing and semantic understanding tasks.\r\n",
          "   - **Natural Language Generation**: Supporting automatic couplet creation, intelligent poem writing, generative Q&A, and open-domain dialogue, reflecting the application of NLP in creative tasks.\r\n",
          "\r\n",
          "### 2. **Industry-grade Model Library**\r\n",
          "   - **Self-developed Pre-trained Models**: Listing multiple ERNIE series models (such as ERNIE-1.0, ERNIE-2.0, ERNIE-Tiny, etc.) as well as models like PLATO-2 and SKEP, demonstrating the diversity of PaddlePaddle in the field of pre-trained models.\r\n",
          "   - **Covering Full-scenario Applications**: Including text classification, matching, generation, semantic indexing, few-shot learning, text graph learning, information extraction, translation, knowledge association, model compression, etc., showcasing the wide applicability of the model library.\r\n",
          "\r\n",
          "### 3. **Core APIs in Text Domain**\r\n",
          "   - Providing basic tool modules, including data management (Data/Datasets), embedding (Embedding), Transformer framework, sequence-to-vector (Seq2Vec), evaluation metrics (Metrics), and loss functions (Losses), offering flexible development interfaces for developers.\r\n",
          "\r\n",
          "### Bottom Logo\r\n",
          "   - The PaddlePaddle logo at the bottom strengthens brand consistency, reflecting the overall ecosystem of PaddlePaddle as an open-source deep learning platform.\r\n",
          "\r\n",
          "**Core Value**: Through predefined tasks, pre-trained models, and core APIs, PaddleNLP has built a complete NLP solution from basic tools to industry applications, balancing efficiency and flexibility, and is suitable for developers to quickly implement natural language processing projects."
        ]
      }
    ],
    "source": [
      "import openai\n",
      "import base64\n",
      "\n",
      "def encode_image(image_path):\n",
      "    with open(image_path, \"rb\") as image_file:\n",
      "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
      "\n",
      "client = openai.Client(\n",
      "    base_url=\"http://0.0.0.0:8180/v1\",\n",
      "    api_key=\"null\"\n",
      ")\n",
      "\n",
      "response = client.chat.completions.create(\n",
      "    model=\"null\",\n",
      "    messages=[\n",
      "        {\n",
      "            \"role\": \"user\",\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"type\": \"image_url\",\n",
      "                    \"image_url\": {\n",
      "                        \"url\": f\"data:image/jpeg;base64,{encode_image('1.jpg')}\"\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"type\": \"text\",\n",
      "                    \"text\": \"生成这张图片的描述\"\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    stream=True,\n",
      ")\n",
      "\n",
      "print(\"图片描述：\", end='', flush=True)\n",
      "for chunk in response:\n",
      "    if chunk.choices and chunk.choices[0].delta.content:\n",
      "        print(chunk.choices[0].delta.content, end='', flush=True)\n",
      "print()"
    ]
  },
  {
    "cell_type": "code",
    "execution_count": 4,
    "metadata": {
      "execution": {
        "iopub.execute_input": "2025-07-07T17:34:50.762852Z",
        "iopub.status.busy": "2025-07-07T17:34:50.762356Z",
        "iopub.status.idle": "2025-07-07T17:34:52.391703Z",
        "shell.execute_reply": "2025-07-07T17:34:52.391041Z",
        "shell.execute_reply.started": "2025-07-07T17:34:50.762817Z"
      },
      "scrolled": true,
      "tags": []
    },
    "outputs": [],
    "source": [
      "%%capture\n",
      "pip install -r requirements.txt --user"
    ]
  },
  {
    "cell_type": "code",
    "execution_count": 5,
    "metadata": {
      "execution": {
        "iopub.execute_input": "2025-07-07T06:21:52.482043Z",
        "iopub.status.busy": "2025-07-07T06:21:52.481699Z",
        "iopub.status.idle": "2025-07-07T06:21:55.461737Z",
        "shell.execute_reply": "2025-07-07T06:21:55.461135Z",
        "shell.execute_reply.started": "2025-07-07T06:21:52.482022Z"
      },
      "scrolled": true,
      "tags": []
    },
    "outputs": [
      {
        "name": "stderr",
        "output_type": "stream",
        "text": [
          "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
          "To disable this warning, you can either:\r\n",
          "\t- Avoid using `tokenizers` before the fork if possible\r\n",
          "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n"
        ]
      },
      {
        "name": "stdout",
        "output_type": "stream",
        "text": [
          "✅ Successfully added 3 records to the symptoms collection\r\n",
          "✅ Successfully added 3 records to the diseases collection\r\n",
          "✅ Successfully added 3 records to the treatments collection\r\n",
          "Medical knowledge base initialization completed!\r\n"
        ]
      }
    ],
    "source": [
      "!python init_knowledge_base.py"
    ]
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "### Initialization failed, please download the embedding model first"
    ]
  },
  {
    "cell_type": "code",
    "execution_count": 3,
    "metadata": {
      "execution": {
        "iopub.execute_input": "2025-07-07T06:21:25.936880Z",
        "iopub.status.busy": "2025-07-07T06:21:25.936587Z",
        "iopub.status.idle": "2025-07-07T06:21:34.351247Z",
        "shell.execute_reply": "2025-07-07T06:21:34.350582Z",
        "shell.execute_reply.started": "2025-07-07T06:21:25.936861Z"
      },
      "scrolled": true,
      "tags": []
    },
    "outputs": [
      {
        "name": "stdout",
        "output_type": "stream",
        "text": [
          "--2025-07-07 14:21:26--  https://chroma-onnx-models.s3.amazonaws.com/all-MiniLM-L6-v2/onnx.tar.gz\r\n",
          "Resolving chroma-onnx-models.s3.amazonaws.com (chroma-onnx-models.s3.amazonaws.com)... 52.217.143.65, 3.5.27.174, 3.5.25.41, ...\r\n",
          "Connecting to chroma-onnx-models.s3.amazonaws.com (chroma-onnx-models.s3.amazonaws.com)|52.217.143.65|:443... connected.\r\n",
          "HTTP request sent, awaiting response... 200 OK\r\n",
          "Length: 83178821 (79M) [application/x-gzip]\r\n",
          "Saving to: '/home/aistudio/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz'\r\n",
          "\r\n",
          "/home/aistudio/.cac 100%[===================>]  79.33M  13.7MB/s    in 7.0s    \r\n",
          "\r\n",
          "2025-07-07 14:21:34 (11.3 MB/s) - '/home/aistudio/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz' saved [83178821/83178821]\r\n",
          "\r\n"
        ]
      }
    ],
    "source": [
      "!wget -O ~/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz \\\n",
      "    https://chroma-onnx-models.s3.amazonaws.com/all-MiniLM-L6-v2/onnx.tar.gz"
    ]
  }
],
"metadata": {
  "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "py35-paddle1.2.0"
  },
  "language_info": {
    "codemirror_mode": {
      "name": "ipython",
      "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.10.10"
  }
},
"nbformat": 4,
"nbformat_minor": 4
}
