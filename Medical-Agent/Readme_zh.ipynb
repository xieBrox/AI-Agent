{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【ERNIE-4.5-VL-28B】基于ERNIE-4.5-VL-28B-Paddle本地化部署+RAG+多Agent协同的多模态智能医疗问诊系统\n",
    "\n",
    "## 项目概述\n",
    "\n",
    "本项目基于**本地化部署的ERNIE-4.5-VL-28B-A3B-Paddle**多模态大模型，构建了一个集成RAG知识库检索与多Agent协同机制的智能医疗问诊系统。通过FastDeploy框架实现模型的高效本地部署，结合ChromaDB知识库和多模态理解能力，为用户提供专业的医疗咨询服务。\n",
    "\n",
    "### 🎯 项目亮点\n",
    "\n",
    "- **🏥 完整医疗场景**：从症状描述到治疗建议的全流程智能问诊\n",
    "- **🖼️ 多模态融合**：支持文本+图像的混合输入，可分析皮肤病变等医疗图像  \n",
    "- **🧠 本地化部署**：基于ERNIE-4.5-VL-28B-A3B-Paddle的完全本地化方案，数据安全可控\n",
    "- **📚 知识库驱动**：ChromaDB构建的医学知识库，支持症状、疾病、治疗方案的智能检索\n",
    "- **🤖 多Agent协同**：症状解析、知识检索、诊断决策等专业Agent的协同工作\n",
    "- **⚡ 高性能推理**：FastDeploy加速框架，单机多卡部署，推理延迟优化\n",
    "\n",
    "### 🏗️ 系统架构图\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[用户问诊输入] --> B[Gradio前端界面]\n",
    "    B --> C[MedicalConsultation]\n",
    "    C --> D[AgentCoordinator协调器]\n",
    "    \n",
    "    D --> E[ERNIE-4.5-VL本地模型]\n",
    "    E --> F[FastDeploy推理引擎]\n",
    "    F --> G[多模态理解]\n",
    "    \n",
    "    D --> H[SymptomParserAgent]\n",
    "    D --> I[KnowledgeRetrievalAgent] \n",
    "    D --> J[DiagnosisAgent]\n",
    "    \n",
    "    I --> K[ChromaDB知识库]\n",
    "    K --> L[症状库]\n",
    "    K --> M[疾病库]\n",
    "    K --> N[治疗库]\n",
    "    \n",
    "    H --> O[症状提取]\n",
    "    I --> P[知识检索]\n",
    "    J --> Q[风险评估]\n",
    "    J --> R[治疗建议]\n",
    "    \n",
    "    O --> S[诊断结果整合]\n",
    "    P --> S\n",
    "    Q --> S\n",
    "    R --> S\n",
    "    \n",
    "    S --> T[结构化医疗报告]\n",
    "    T --> B\n",
    "```\n",
    "\n",
    "### 🏆 技术创新总结\n",
    "\n",
    "本项目实现了从**大模型本地化部署**到**智能医疗应用**的完整技术链路：\n",
    "\n",
    "1. **🔥 核心突破**：28B参数ERNIE-4.5-VL多模态大模型的高效本地化部署\n",
    "2. **🧠 智能升级**：RAG知识库检索 + 多Agent协同的医疗专家系统  \n",
    "3. **🛡️ 数据安全**：完全本地化方案，患者隐私零泄露\n",
    "4. **⚡ 性能优化**：FastDeploy推理加速，秒级响应医疗问诊\n",
    "\n",
    "### 🛠️ 技术栈选择\n",
    "\n",
    "| 层级 | 技术组件 | 版本 | 作用 |\n",
    "|------|---------|------|------|\n",
    "| **AI模型层** | ERNIE-4.5-VL-28B-A3B-Paddle | 28B参数 | 多模态理解与生成 |\n",
    "| **推理框架** | FastDeploy | 最新版 | 模型部署与推理加速 |\n",
    "| **知识库** | ChromaDB | 1.0.15 | 向量数据库与语义检索 |\n",
    "| **Web框架** | Gradio | 5.35.0 | 交互式用户界面 |\n",
    "| **Agent框架** | 自研多Agent系统 | - | 任务协调与业务逻辑 |\n",
    "| **数据处理** | Pillow + NumPy | 10.2.0 + 1.24.3 | 图像处理与数值计算 |\n",
    "\n",
    "## 🏥 智能医疗问诊系统实现\n",
    "\n",
    "### 核心功能模块\n",
    "\n",
    "#### 1. 多模态输入处理\n",
    "```python\n",
    "class ErnieClient:\n",
    "    def medical_image_analysis(self, image_path: str) -> str:\n",
    "        \"\"\"医疗图像分析\"\"\"\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "        \n",
    "        messages = [{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                {\"type\": \"text\", \"text\": \"请分析这张医疗图像，描述可见的症状特征\"}\n",
    "            ]\n",
    "        }]\n",
    "        # 调用本地ERNIE-4.5-VL模型\n",
    "        return self._call_local_model(messages)\n",
    "```\n",
    "\n",
    "#### 2. 知识库检索系统\n",
    "```python\n",
    "class KnowledgeBase:\n",
    "    def __init__(self, persist_directory=\"medical_kb\"):\n",
    "        # 使用ChromaDB构建医学知识库\n",
    "        self.client = chromadb.PersistentClient(path=persist_directory)\n",
    "        \n",
    "        # 创建专业医学集合\n",
    "        self.symptoms_collection = self.client.get_or_create_collection(\"symptoms\")\n",
    "        self.diseases_collection = self.client.get_or_create_collection(\"diseases\")  \n",
    "        self.treatments_collection = self.client.get_or_create_collection(\"treatments\")\n",
    "```\n",
    "\n",
    "#### 3. 多Agent协同系统\n",
    "```python\n",
    "class AgentCoordinator:\n",
    "    def process_consultation(self, text_input: str, image_path: str = None):\n",
    "        # 1. 图像分析（如果提供）\n",
    "        image_analysis = self.ernie.medical_image_analysis(image_path) if image_path else None\n",
    "        \n",
    "        # 2. 症状解析Agent\n",
    "        symptoms = self.symptom_parser.parse_symptoms(text_input, image_analysis)\n",
    "        \n",
    "        # 3. 知识检索Agent  \n",
    "        medical_info = self.knowledge_retriever.retrieve_relevant_info(symptoms)\n",
    "        \n",
    "        # 4. 诊断决策Agent\n",
    "        risk_assessment = self.diagnosis_agent.analyze_risk_level(symptoms, medical_info)\n",
    "        treatment_plan = self.diagnosis_agent.generate_treatment_plan(symptoms, medical_info)\n",
    "        \n",
    "        return {\n",
    "            \"symptoms\": symptoms,\n",
    "            \"risk_assessment\": risk_assessment, \n",
    "            \"treatment_plan\": treatment_plan,\n",
    "            \"image_analysis\": image_analysis\n",
    "        }\n",
    "```\n",
    "\n",
    "### 🎯 应用场景与效果\n",
    "\n",
    "#### 典型使用流程\n",
    "1. **用户输入**：描述症状 + 上传病变图片（可选）\n",
    "2. **多模态分析**：ERNIE-4.5-VL同时理解文本和图像\n",
    "3. **症状提取**：AI识别关键症状和医学术语\n",
    "4. **知识检索**：从专业医学库中检索相关疾病信息\n",
    "5. **风险评估**：评估病情严重程度和就医紧急性\n",
    "6. **治疗建议**：生成个性化的检查、用药和生活建议\n",
    "\n",
    "#### 系统优势\n",
    "- **数据安全**：完全本地化部署，患者数据不出本地\n",
    "- **专业准确**：基于28B参数的专业医学模型\n",
    "- **响应快速**：本地推理，无网络延迟\n",
    "- **持续学习**：知识库可不断扩充和更新\n",
    "\n",
    "## 🧠 ERNIE-4.5-VL-28B-A3B-Paddle模型选择\n",
    "\n",
    "### 为什么选择ERNIE-4.5-VL-28B-A3B-Paddle？\n",
    "\n",
    "#### 文心开源模型的选择\n",
    "\n",
    "### ERNIE-4.5模型系列规格对比表\n",
    "\n",
    "| 模型系列 | 模型名称 | 总参数 | 激活参数 | 模态支持 | 上下文长度 | 主要用途 | 部署场景 |\n",
    "|---------|---------|--------|---------|---------|-----------|---------|---------|\n",
    "| **A47B大规模** | ERNIE-4.5-300B-A47B-Base | 300B | 47B | 文本 | 128K | 预训练基座 | 云端GPU集群 |\n",
    "| | ERNIE-4.5-300B-A47B | 300B | 47B | 文本 | 128K | 指令遵循/创意生成 | 云端GPU集群 |\n",
    "| | ERNIE-4.5-VL-424B-A47B-Base | 424B | 47B | 文本+视觉 | 128K | 多模态预训练 | 云端GPU集群 |\n",
    "| | ERNIE-4.5-VL-424B-A47B | 424B | 47B | 文本+视觉 | 128K | 图文理解/生成 | 云端GPU集群 |\n",
    "| **A3B中等规模** | ERNIE-4.5-21B-A3B-Base | 21B | 3B | 文本 | 128K | 预训练基座 | 单机多卡 |\n",
    "| | ERNIE-4.5-21B-A3B | 21B | 3B | 文本 | 128K | 对话/文档处理 | 单机多卡 |\n",
    "| | ERNIE-4.5-VL-28B-A3B-Base | 28B | 3B | 文本+视觉 | 128K | 多模态预训练 | 单机多卡 |\n",
    "| | ERNIE-4.5-VL-28B-A3B | 28B | 3B | 文本+视觉 | 128K | 轻量多模态应用 | 单机多卡 |\n",
    "| **0.3B轻量** | ERNIE-4.5-0.3B-Base | 0.3B | 0.3B | 文本 | 4K | 端侧预训练 | 移动端/边缘 |\n",
    "| | ERNIE-4.5-0.3B | 0.3B | 0.3B | 文本 | 4K | 实时对话 | 移动端/边缘 |\n",
    "\n",
    "### 模型规格选择策略表\n",
    "\n",
    "| 应用场景 | 推荐模型 | 理由 | 硬件要求 | 推理延迟 |\n",
    "|---------|---------|------|---------|---------|\n",
    "| **复杂推理任务** | ERNIE-4.5-300B-A47B | 最强推理能力 | 8×A100(80GB) | 高 |\n",
    "| **创意内容生成** | ERNIE-4.5-300B-A47B | 最佳创意表现 | 8×A100(80GB) | 高 |\n",
    "| **多模态理解** | ERNIE-4.5-VL-424B-A47B | 图文融合理解 | 8×A100(80GB) | 高 |\n",
    "| **日常对话客服** | ERNIE-4.5-21B-A3B | 性能成本平衡 | 4×V100(32GB) | 中 |\n",
    "| **文档信息抽取** | ERNIE-4.5-21B-A3B | 理解能力充足 | 4×V100(32GB) | 中 |\n",
    "| **轻量多模态** | ERNIE-4.5-VL-28B-A3B | 图文处理均衡 | 4×V100(32GB) | 中 |\n",
    "| **移动端应用** | ERNIE-4.5-0.3B | 低延迟快响应 | 1×GPU/CPU | 低 |\n",
    "| **边缘计算** | ERNIE-4.5-0.3B | 资源消耗最小 | CPU/NPU | 低 |\n",
    "\n",
    "### 为什么选择ERNIE-4.5-VL-28B-A3B-Paddle模型？\n",
    "\n",
    "#### 1. **性能与成本的最佳平衡**\n",
    "- **参数规模适中**：28B总参数，3B激活参数，在保证推理能力的同时控制了计算成本\n",
    "- **硬件要求合理**：支持单机多卡部署（4×V100或2×A100），相比A47B系列降低了75%的硬件门槛\n",
    "- **推理延迟适中**：在保证输出质量的前提下，响应速度比大规模模型快3-5倍\n",
    "\n",
    "#### 2. **多模态能力突出**\n",
    "- **文本+视觉融合**：原生支持图文理解，无需额外的视觉编码器\n",
    "- **长上下文支持**：128K token上下文长度，可处理长文档和多张图片\n",
    "- **应用场景丰富**：适合文档分析、图像描述、多模态问答等任务\n",
    "\n",
    "#### 3. **部署友好性**\n",
    "- **AIStudio平台优化**：官方深度适配，提供一键下载和部署\n",
    "- **FastDeploy集成**：完整的推理加速和服务化支持\n",
    "- **开源生态**：PaddlePaddle生态系统，文档完善，社区活跃\n",
    "\n",
    "#### 4. **实际应用价值**\n",
    "- **企业级可用**：相比0.3B模型，理解能力和生成质量显著提升\n",
    "- **成本可控**：相比A47B系列，部署成本降低70%，运营成本降低60%\n",
    "- **扩展性强**：支持LoRA微调，可针对特定场景进行优化\n",
    "\n",
    "#### 5. **技术先进性**\n",
    "- **MoE架构**：专家混合模型，激活参数仅为总参数的1/9，推理效率高\n",
    "- **多模态对齐**：视觉和文本特征深度融合，理解能力接近GPT-4V\n",
    "- **中文优化**：针对中文场景深度优化，在中文多模态任务上表现优异\n",
    "\n",
    "#### 选择建议\n",
    "**推荐场景**：\n",
    "- 中小企业多模态AI应用开发\n",
    "- 教育科研项目的多模态实验\n",
    "- 个人开发者的AI产品原型验证\n",
    "- 需要图文理解能力的业务系统\n",
    "\n",
    "**不推荐场景**：\n",
    "- 对推理延迟要求极高的实时系统（选择0.3B）\n",
    "- 预算充足且追求极致性能的场景（选择A47B）\n",
    "- 纯文本应用且对多模态无需求（选择21B-A3B）\n",
    "\n",
    "## 🚀 ERNIE-4.5-VL模型本地化部署方案\n",
    "\n",
    "### 一、环境准备\n",
    "### 1. 硬件要求\n",
    "- **GPU**：NVIDIA A100 80GB（支持单卡/多卡，推荐CUDA 11.8+）  \n",
    "- **内存**：≥60GB RAM  \n",
    "- **存储**：≥60GB（模型约28GB，需预留日志/缓存空间）  \n",
    "\n",
    "### 2. 软件依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T13:00:11.836046Z",
     "iopub.status.busy": "2025-07-06T13:00:11.835593Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m pip install fastdeploy-gpu -i https://www.paddlepaddle.org.cn/packages/stable/fastdeploy-gpu-80_90/ --extra-index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 环境验证\n",
    "#### ① 检查CUDA版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T17:25:11.092125Z",
     "iopub.status.busy": "2025-07-07T17:25:11.091792Z",
     "iopub.status.idle": "2025-07-07T17:25:11.310890Z",
     "shell.execute_reply": "2025-07-07T17:25:11.310222Z",
     "shell.execute_reply.started": "2025-07-07T17:25:11.092105Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\r\n",
      "Built on Tue_Oct_29_23:50:19_PDT_2024\r\n",
      "Cuda compilation tools, release 12.6, V12.6.85\r\n",
      "Build cuda_12.6.r12.6/compiler.35059454_0\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "# 预期输出示例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ② 检查GPU信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T17:25:27.824496Z",
     "iopub.status.busy": "2025-07-07T17:25:27.824188Z",
     "iopub.status.idle": "2025-07-07T17:25:28.730838Z",
     "shell.execute_reply": "2025-07-07T17:25:28.730260Z",
     "shell.execute_reply.started": "2025-07-07T17:25:27.824477Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  8 01:25:28 2025       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  NVIDIA A800-SXM4-80GB          On  |   00000000:D3:00.0 Off |                    0 |\r\n",
      "| N/A   41C    P0             66W /  400W |       0MiB /  81920MiB |      0%      Default |\r\n",
      "|                                         |                        |             Disabled |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# 检查GPU状态和显存\n",
    "!nvidia-smi\n",
    "# 预期输出示例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ③ 常见环境问题排查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 检查PaddlePaddle GPU支持\n",
    "!python -c \"import paddle; print('GPU可用:', paddle.is_compiled_with_cuda()); print('GPU设备数:', paddle.device.cuda.device_count())\"\n",
    "\n",
    "# 检查FastDeploy安装\n",
    "!python -c \"from fastdeploy import LLM, SamplingParams; print('FastDeploy安装成功！')\"\n",
    "\n",
    "# 检查OpenAI库版本\n",
    "!python -c \"import openai; print('OpenAI库版本:', openai.__version__)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、模型下载与目录结构\n",
    "### 1. 下载模型文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# 使用AIStudio命令下载模型\n",
    "!aistudio download --model PaddlePaddle/ERNIE-4.5-VL-28B-A3B-Paddle --local_dir /home/aistudio/work/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T17:28:11.223932Z",
     "iopub.status.busy": "2025-07-07T17:28:11.223616Z",
     "iopub.status.idle": "2025-07-07T17:28:11.427076Z",
     "shell.execute_reply": "2025-07-07T17:28:11.426503Z",
     "shell.execute_reply.started": "2025-07-07T17:28:11.223912Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 57441133\r\n",
      "-rw-r--r-- 1 aistudio aistudio      11366 Jul  6 18:57 LICENSE\r\n",
      "-rw-r--r-- 1 aistudio aistudio       9077 Jul  6 18:56 README.md\r\n",
      "-rw-r--r-- 1 aistudio aistudio      86904 Jul  6 18:56 added_tokens.json\r\n",
      "-rw-r--r-- 1 aistudio aistudio       1306 Jul  6 18:57 config.json\r\n",
      "-rw-r--r-- 1 aistudio aistudio        134 Jul  6 18:57 generation_config.json\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4991326368 Jul  6 18:57 model-00001-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4988696384 Jul  6 18:56 model-00002-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4999185600 Jul  6 18:56 model-00003-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4995268296 Jul  6 18:57 model-00004-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4988696984 Jul  6 18:56 model-00005-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4999193256 Jul  6 18:57 model-00006-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4995261896 Jul  6 18:57 model-00007-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4999183248 Jul  6 18:56 model-00008-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4995271816 Jul  6 18:57 model-00009-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4988696992 Jul  6 18:57 model-00010-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 4999190216 Jul  6 18:57 model-00011-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio 3877260800 Jul  6 18:57 model-00012-of-00012.safetensors\r\n",
      "-rw-r--r-- 1 aistudio aistudio     691887 Jul  6 18:56 model.safetensors.index.json\r\n",
      "-rw-r--r-- 1 aistudio aistudio        615 Jul  6 18:56 preprocessor_config.json\r\n",
      "-rw-r--r-- 1 aistudio aistudio      62694 Jul  6 18:56 special_tokens_map.json\r\n",
      "-rw-r--r-- 1 aistudio aistudio    1614362 Jul  6 18:56 tokenizer.model\r\n",
      "-rw-r--r-- 1 aistudio aistudio       3606 Jul  6 18:56 tokenizer_config.json\r\n"
     ]
    }
   ],
   "source": [
    "# 查看模型文件\n",
    "!ls -l work/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 目录结构\n",
    "```\n",
    "work/\n",
    "└── models/\n",
    "    ├── LICENSE                           # 许可证文件\n",
    "    ├── README.md                         # 模型说明文档\n",
    "    ├── added_tokens.json                 # 新增token配置\n",
    "    ├── config.json                       # 模型配置文件\n",
    "    ├── generation_config.json            # 生成配置文件\n",
    "    ├── model-00001-of-00012.safetensors  # 模型参数文件(分片1/12)\n",
    "    ├── model-00002-of-00012.safetensors  # 模型参数文件(分片2/12)\n",
    "    ├── model-00003-of-00012.safetensors  # 模型参数文件(分片3/12)\n",
    "    ├── model-00004-of-00012.safetensors  # 模型参数文件(分片4/12)\n",
    "    ├── model-00005-of-00012.safetensors  # 模型参数文件(分片5/12)\n",
    "    ├── model-00006-of-00012.safetensors  # 模型参数文件(分片6/12)\n",
    "    ├── model-00007-of-00012.safetensors  # 模型参数文件(分片7/12)\n",
    "    ├── model-00008-of-00012.safetensors  # 模型参数文件(分片8/12)\n",
    "    ├── model-00009-of-00012.safetensors  # 模型参数文件(分片9/12)\n",
    "    ├── model-00010-of-00012.safetensors  # 模型参数文件(分片10/12)\n",
    "    ├── model-00011-of-00012.safetensors  # 模型参数文件(分片11/12)\n",
    "    ├── model-00012-of-00012.safetensors  # 模型参数文件(分片12/12)\n",
    "    ├── model.safetensors.index.json      # 模型分片索引文件\n",
    "    ├── preprocessor_config.json          # 预处理器配置\n",
    "    ├── special_tokens_map.json           # 特殊token映射\n",
    "    ├── tokenizer.model                   # 分词器模型文件\n",
    "    └── tokenizer_config.json             # 分词器配置文件\n",
    "```\n",
    "\n",
    "### 3. 文件说明\n",
    "| 文件类型 | 文件名 | 说明 |\n",
    "|---------|--------|------|\n",
    "| **模型权重** | model-00001~00012-of-00012.safetensors | 模型参数分片文件，安全张量格式 |\n",
    "| **索引文件** | model.safetensors.index.json | 模型分片索引，指定每个参数在哪个分片中 |\n",
    "| **配置文件** | config.json | 模型架构配置，包含层数、隐藏层大小等 |\n",
    "| **生成配置** | generation_config.json | 文本生成相关配置，如最大长度、采样参数等 |\n",
    "| **分词器** | tokenizer.model | SentencePiece分词器模型 |\n",
    "| **分词器配置** | tokenizer_config.json | 分词器配置参数 |\n",
    "| **预处理器** | preprocessor_config.json | 图像预处理配置（多模态模型专用） |\n",
    "| **特殊token** | special_tokens_map.json | 特殊标记映射，如padding、unknown等 |\n",
    "| **新增token** | added_tokens.json | 用户自定义添加的token |\n",
    "\n",
    "## 三、启动服务（关键命令）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T10:57:33.976246Z",
     "iopub.status.busy": "2025-07-06T10:57:33.975924Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\r\n",
      "  warnings.warn(warning_message)\r\n",
      "/home/aistudio/external-libraries/lib/python3.10/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\r\n",
      "  warnings.warn(\r\n",
      "\u001b[32m[2025-07-06 18:57:55,035] [    INFO]\u001b[0m - loading configuration file work/models/preprocessor_config.json\u001b[0m\r\n",
      "INFO     2025-07-06 18:57:58,188 5574  engine.py[line:206] Waitting worker processes ready...\r\n",
      "Loading Weights: 100%|████████████████████████| 100/100 [01:17<00:00,  1.30it/s]\r\n",
      "Loading Layers: 100%|█████████████████████████| 100/100 [00:07<00:00, 13.31it/s]\r\n",
      "INFO     2025-07-06 18:59:35,857 5574  engine.py[line:276] Worker processes are launched with 119.6253821849823 seconds.\r\n",
      "INFO     2025-07-06 18:59:35,857 5574  api_server.py[line:91] Launching metrics service at http://0.0.0.0:8181/metrics\r\n",
      "INFO     2025-07-06 18:59:35,857 5574  api_server.py[line:94] Launching chat completion service at http://0.0.0.0:8180/v1/chat/completions\r\n",
      "INFO     2025-07-06 18:59:35,857 5574  api_server.py[line:97] Launching completion service at http://0.0.0.0:8180/v1/completions\r\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m5574\u001b[0m]\r\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\r\n",
      "\u001b[32m[2025-07-06 18:59:54,481] [    INFO]\u001b[0m - loading configuration file work/models/preprocessor_config.json\u001b[0m\r\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\r\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8180\u001b[0m (Press CTRL+C to quit)\r\n"
     ]
    }
   ],
   "source": [
    "python -m fastdeploy.entrypoints.openai.api_server \\\n",
    "       --model work/models \\\n",
    "       --port 8180 \\\n",
    "       --metrics-port 8181 \\\n",
    "       --engine-worker-queue-port 8182 \\\n",
    "       --max-model-len 32768 \\\n",
    "       --enable-mm \\\n",
    "       --reasoning-parser ernie-45-vl \\\n",
    "       --max-num-seqs 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 服务启动参数详细说明\n",
    "\n",
    "| 参数名称 | 参数说明 | 默认值 | 示例值 |\n",
    "|---------|---------|-------|-------|\n",
    "| `--model` | 模型文件路径，包含模型权重和配置文件的目录 | 必选 | `work/models` |\n",
    "| `--port` | API服务监听端口，客户端通过此端口访问服务 | 8000 | `8180` |\n",
    "| `--metrics-port` | 监控指标服务端口，用于性能监控和健康检查 | 8001 | `8181` |\n",
    "| `--engine-worker-queue-port` | 引擎工作队列端口，用于内部任务调度 | 8002 | `8182` |\n",
    "| `--max-model-len` | 模型最大支持的序列长度（token数） | 2048 | `32768` |\n",
    "| `--enable-mm` | 启用多模态功能（文本+图像处理） | False | `开启` |\n",
    "| `--reasoning-parser` | 推理解析器类型，指定模型的推理逻辑 | 无 | `ernie-45-vl` |\n",
    "| `--max-num-seqs` | 最大并发序列数，控制批处理大小 | 256 | `32` |\n",
    "\n",
    "### 其他常用参数\n",
    "\n",
    "| 参数名称 | 参数说明 | 默认值 | 备注 |\n",
    "|---------|---------|-------|------|\n",
    "| `--host` | 服务绑定的主机地址 | 0.0.0.0 | 设置为0.0.0.0允许外部访问 |\n",
    "| `--trust-remote-code` | 信任远程代码执行 | False | 加载自定义模型时需要 |\n",
    "| `--tensor-parallel-size` | 张量并行大小（多GPU） | 1 | 根据GPU数量设置 |\n",
    "| `--gpu-memory-utilization` | GPU内存利用率 | 0.9 | 建议0.8-0.95之间 |\n",
    "| `--max-num-batched-tokens` | 最大批处理token数 | 自动计算 | 根据GPU显存调整 |\n",
    "| `--swap-space` | 交换空间大小(GB) | 4 | 内存不足时使用 |\n",
    "| `--enable-lora` | 启用LoRA适配器 | False | 微调模型时使用 |\n",
    "| `--max-log-len` | 最大日志长度 | 无限制 | 控制日志文件大小 |\n",
    "\n",
    "### 服务验证\n",
    "```bash\n",
    "# 检查模型是否正常加载\n",
    "curl http://localhost:8180/v1/models\n",
    "\n",
    "# 预期输出（包含模型ID）\n",
    "{\"data\":[{\"id\":\"ernie-4.5-vl-28b-a3b-paddle\",\"object\":\"model\"}]}\n",
    "```\n",
    "\n",
    "## 四、模型调用示例\n",
    "\n",
    "### 1. OpenAI库调用方式\n",
    "\n",
    "#### ① 文本生成\n",
    "```python\n",
    "import openai\n",
    "\n",
    "host = \"0.0.0.0\"\n",
    "port = \"8180\"\n",
    "client = openai.Client(base_url=f\"http://{host}:{port}/v1\", api_key=\"null\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"null\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"你是Aistudio和文心大模型开发的智能助手，请介绍一下你自己.\"}\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta:\n",
    "        print(chunk.choices[0].delta.content, end='')\n",
    "```\n",
    "\n",
    "#### ② 图像描述生成\n",
    "```python\n",
    "import openai\n",
    "import base64\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "client = openai.Client(\n",
    "    base_url=\"http://0.0.0.0:8180/v1\",\n",
    "    api_key=\"null\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"null\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image('1.jpg')}\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"生成这张图片的描述\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"图片描述：\", end='', flush=True)\n",
    "for chunk in response:\n",
    "    if chunk.choices and chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end='', flush=True)\n",
    "print()\n",
    "```\n",
    "\n",
    "### 2. requests库调用方式\n",
    "\n",
    "#### ① 文本生成\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:8180/v1/chat/completions\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "    \"model\": \"null\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"你是Aistudio和文心大模型开发的智能助手，请介绍一下你自己.\"}\n",
    "    ],\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload, stream=True)\n",
    "\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        line = line.decode('utf-8').replace('data: ', '')\n",
    "        \n",
    "        if line.strip() == '[DONE]':\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            \n",
    "            if 'choices' in data and len(data['choices']) > 0:\n",
    "                delta = data['choices'][0].get('delta', {})\n",
    "                content = delta.get('content', '')\n",
    "                if content:\n",
    "                    print(content, end='', flush=True)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding line: {line}\")\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "print()\n",
    "```\n",
    "\n",
    "#### ② 图像描述生成\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "url = \"http://localhost:8180/v1/chat/completions\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "    \"model\": \"null\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image('1.jpg')}\"}},\n",
    "                {\"type\": \"text\", \"text\": \"生成图片描述\"}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload, stream=True)\n",
    "\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        line = line.decode('utf-8').replace('data: ', '')\n",
    "        \n",
    "        if line.strip() == '[DONE]':\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            \n",
    "            if 'choices' in data and len(data['choices']) > 0:\n",
    "                delta = data['choices'][0].get('delta', {})\n",
    "                content = delta.get('content', '')\n",
    "                if content:\n",
    "                    print(content, end='', flush=True)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding line: {line}\")\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "print()\n",
    "```\n",
    "\n",
    "### 3. 调用参数说明\n",
    "\n",
    "#### 常用参数配置\n",
    "```python\n",
    "# 完整的调用参数示例\n",
    "response = client.chat.completions.create(\n",
    "    model=\"null\",                    # 模型名称（固定值）\n",
    "    messages=[...],                  # 消息列表\n",
    "    stream=True,                     # 是否启用流式响应\n",
    "    max_tokens=2048,                 # 最大生成token数\n",
    "    temperature=0.7,                 # 温度参数(0.0-2.0)\n",
    "    top_p=0.9,                      # 核采样参数(0.0-1.0)\n",
    "    frequency_penalty=0.0,           # 频率惩罚(-2.0-2.0)\n",
    "    presence_penalty=0.0,            # 存在惩罚(-2.0-2.0)\n",
    "    stop=[\"<|endoftext|>\"],         # 停止词列表\n",
    ")\n",
    "```\n",
    "\n",
    "#### 参数详细说明\n",
    "| 参数名称 | 类型 | 默认值 | 说明 |\n",
    "|---------|------|--------|------|\n",
    "| `model` | str | \"null\" | 模型名称，本地部署时固定为\"null\" |\n",
    "| `messages` | list | 必选 | 对话消息列表，包含role和content |\n",
    "| `stream` | bool | False | 是否启用流式响应 |\n",
    "| `max_tokens` | int | 自动 | 最大生成token数量 |\n",
    "| `temperature` | float | 1.0 | 控制随机性，值越高越随机 |\n",
    "| `top_p` | float | 1.0 | 核采样，控制词汇选择范围 |\n",
    "| `frequency_penalty` | float | 0.0 | 频率惩罚，减少重复内容 |\n",
    "| `presence_penalty` | float | 0.0 | 存在惩罚，鼓励谈论新话题 |\n",
    "| `stop` | list | None | 停止生成的字符串列表 |\n",
    "\n",
    "## 五、常见问题与解决方案\n",
    "### 1. 端口占用问题\n",
    "```bash\n",
    "# 查看端口占用进程\n",
    "lsof -i:8180\n",
    "\n",
    "# 终止进程（替换<PID>为实际进程号）\n",
    "kill -9 <PID>\n",
    "```\n",
    "\n",
    "### 2. 模型加载失败\n",
    "- **检查目录**：确认`work/models`下存在完整的模型文件（.pdparams/config.json/vocab.txt）  \n",
    "- **解析器参数**：确保启动命令包含`--reasoning-parser ernie-45-vl`  \n",
    "- **驱动版本**：NVIDIA驱动需≥520.61.05（支持A100）  \n",
    "\n",
    "### 3. 流式响应异常\n",
    "- 确保`stream=True`参数正确传递  \n",
    "- 检查服务日志（`fastdeploy_server.log`）是否有内存/显存不足报错  \n",
    "\n",
    "\n",
    "## 六、测试用例\n",
    "### 1. 文本生成验证\n",
    "- **输入**：`\"总结本教程的核心步骤\"`  \n",
    "- **预期**：输出包含\"环境配置\"、\"服务启动\"、\"接口调用\"等关键词的连贯文本  \n",
    "\n",
    "### 2. 图像描述验证\n",
    "- **测试图片**：使用包含自然场景的jpg文件（如山水、人物活动）  \n",
    "- **预期**：输出包含景物特征（如\"蓝天白云下的绿色草原\"）、动作描述（如\"人物正在湖边散步\"）的语句  \n",
    "\n",
    "## 七、智能医疗问诊系统部署\n",
    "\n",
    "### 1. 系统依赖安装\n",
    "```bash\n",
    "# 更新依赖包版本\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### 2. 医学知识库初始化\n",
    "```bash\n",
    "# 初始化ChromaDB医学知识库\n",
    "python init_knowledge_base.py\n",
    "\n",
    "# 启动后会创建以下集合：\n",
    "# - symptoms: 症状知识库\n",
    "# - diseases: 疾病知识库  \n",
    "# - treatments: 治疗方案库\n",
    "```\n",
    "\n",
    "### 3. 启动完整系统\n",
    "```bash\n",
    "# 第一步：启动ERNIE-4.5-VL模型服务\n",
    "python -m fastdeploy.entrypoints.openai.api_server \\\n",
    "       --model work/models \\\n",
    "       --port 8180 \\\n",
    "       --enable-mm \\\n",
    "       --reasoning-parser ernie-45-vl\n",
    "\n",
    "# 第二步：启动医疗问诊Web界面  \n",
    "python main.gradio.py\n",
    "```\n",
    "\n",
    "### 4. 系统验证\n",
    "```bash\n",
    "# 运行系统测试\n",
    "python test_system.py\n",
    "\n",
    "# 预期输出：\n",
    "# ✅ ERNIE 服务连接成功\n",
    "# ✅ 知识库连接正常  \n",
    "# ✅ 症状分析功能正常\n",
    "# ✅ 问诊流程完成\n",
    "```\n",
    "\n",
    "## 八、功能展示与效果\n",
    "\n",
    "### 🖼️ 系统界面\n",
    "![智能医疗问诊系统主界面](https://ai-studio-static-online.cdn.bcebos.com/9071eee410474644988213a51be33c75de6f65b21f5b44328d25094600a98361)\n",
    "\n",
    "### 📋 诊断报告示例\n",
    "![多模态问诊结果](https://ai-studio-static-online.cdn.bcebos.com/b899c223a17849f3ae0414b569f480bc24c98fc54fbf4923b57e24eb15850c65)\n",
    "\n",
    "### 🔍 图像分析能力  \n",
    "![皮肤病变图像分析](https://ai-studio-static-online.cdn.bcebos.com/be14a288a7a848a783b700d5292b767eddf11bee1253412c95460e0619dadba2)\n",
    "\n",
    "### 💊 治疗建议输出格式\n",
    "```\n",
    "【症状分析】\n",
    "识别到的症状：发热、咳嗽、乏力\n",
    "\n",
    "【风险评估】  \n",
    "风险等级：⚠️⚠️ (建议普通门诊就医)\n",
    "建议：\n",
    "- 症状持续时间较短，暂无严重并发症\n",
    "- 建议及时就医排查感染性疾病\n",
    "\n",
    "【建议检查项目】\n",
    "- 血常规检查\n",
    "- C反应蛋白检测\n",
    "- 胸部X光片\n",
    "\n",
    "【用药建议】\n",
    "- 对症治疗：布洛芬退热\n",
    "- 止咳化痰：复方甘草片\n",
    "- 请遵医嘱用药，避免自行用药\n",
    "\n",
    "【生活建议】  \n",
    "- 充分休息，避免剧烈运动\n",
    "- 多饮水，保持室内通风\n",
    "- 注意保暖，避免再次受凉\n",
    "```\n",
    "\n",
    "## 九、技术创新点\n",
    "\n",
    "### 🔬 核心技术优势\n",
    "\n",
    "#### 1. 本地化多模态大模型\n",
    "- **模型规模**：28B参数，3B激活参数的高效MoE架构\n",
    "- **多模态能力**：原生支持文本+图像的联合理解\n",
    "- **部署优化**：FastDeploy框架，单机多卡高效推理\n",
    "- **数据安全**：完全本地化，患者隐私零泄露\n",
    "\n",
    "#### 2. RAG增强知识系统\n",
    "- **向量化存储**：ChromaDB构建的高效语义检索\n",
    "- **分层知识库**：症状、疾病、治疗方案的结构化管理\n",
    "- **实时检索**：毫秒级的相似度匹配和知识召回\n",
    "- **动态更新**：支持知识库的增量更新和扩展\n",
    "\n",
    "#### 3. 多Agent协同架构\n",
    "- **模块化设计**：每个Agent负责特定的医疗任务\n",
    "- **智能编排**：AgentCoordinator统一调度和数据流管理\n",
    "- **容错机制**：单个Agent失败不影响整体系统运行\n",
    "- **可扩展性**：新增医疗专科Agent即插即用\n",
    "\n",
    "#### 4. 用户体验优化\n",
    "- **流式响应**：实时显示AI分析过程，提升交互体验\n",
    "- **多端适配**：Web界面支持PC和移动端访问\n",
    "- **结果可视化**：结构化医疗报告，易于理解和保存\n",
    "- **操作便捷**：拖拽上传图片，文本框快速输入\n",
    "\n",
    "## 十、应用价值与场景\n",
    "\n",
    "### 🏥 医疗场景应用\n",
    "\n",
    "#### 基层医疗机构\n",
    "- **初步问诊**：协助全科医生进行症状分析\n",
    "- **分诊辅助**：评估患者病情紧急程度  \n",
    "- **知识支持**：为医生提供疾病诊疗参考\n",
    "\n",
    "#### 远程医疗服务\n",
    "- **在线咨询**：24小时智能医疗咨询服务\n",
    "- **图像诊断**：皮肤病、外伤等可视化疾病分析\n",
    "- **健康教育**：提供专业的健康管理建议\n",
    "\n",
    "#### 个人健康管理\n",
    "- **症状自查**：用户自主进行健康状况评估\n",
    "- **就医指导**：提供科学的就医建议和科室推荐\n",
    "- **用药咨询**：基于症状的安全用药指导\n",
    "\n",
    "### 💡 技术价值\n",
    "\n",
    "#### 行业推动\n",
    "- **AI医疗标准化**：多模态医疗AI的技术范式\n",
    "- **本地化部署**：为医疗数据安全提供解决方案\n",
    "- **开源生态**：基于PaddlePaddle的完整技术栈\n",
    "\n",
    "#### 创新突破\n",
    "- **多模态融合**：图文一体化的医疗理解能力\n",
    "- **知识库驱动**：RAG技术在医疗领域的深度应用\n",
    "- **Agent协同**：专业化AI系统的协作机制\n",
    "\n",
    "## 十一、系统监控与维护\n",
    "\n",
    "### 📊 性能监控\n",
    "```bash\n",
    "# 查看模型服务状态\n",
    "curl http://localhost:8180/v1/models\n",
    "\n",
    "# 监控系统资源使用\n",
    "nvidia-smi  # GPU使用情况\n",
    "top         # CPU和内存使用\n",
    "\n",
    "# 查看服务日志\n",
    "tail -f logs/gradio_app_*.log\n",
    "\n",
    "\n",
    "### 📞 联系方式\n",
    "- **项目作者**：Wechat: X_ruilian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T13:47:22.113188Z",
     "iopub.status.busy": "2025-07-06T13:47:22.112841Z",
     "iopub.status.idle": "2025-07-06T13:47:36.519914Z",
     "shell.execute_reply": "2025-07-06T13:47:36.519349Z",
     "shell.execute_reply.started": "2025-07-06T13:47:22.113166Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 21:47:22,130 - INFO - HTTP Request: POST http://0.0.0.0:8180/v1/chat/completions \"HTTP/1.1 200 OK\"\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "我是由 **Aistudio**（百度开源的AI开发平台）与 **文心大模型** 技术框架共同赋能的智能助手。以下是我的核心特点与功能介绍：\r\n",
      "\r\n",
      "### 1. **技术背景**\r\n",
      "   - 基于百度自主研发的 **文心大模型** 基础能力，融合多模态理解与生成技术，具备强大的自然语言处理能力。\r\n",
      "   - 依托Aistudio的开源生态，支持开发者协作与模型迭代优化。\r\n",
      "\r\n",
      "### 2. **核心能力**\r\n",
      "   - **知识问答**：覆盖广泛领域（科技、文化、生活等），提供准确、简洁的解答。\r\n",
      "   - **文本生成**：可撰写文章、代码、诗歌、对话等，支持创意与实用场景。\r\n",
      "   - **逻辑推理**：分析复杂问题，提供结构化思考路径。\r\n",
      "   - **多语言支持**：中文为主，兼顾英文等语言交互。\r\n",
      "\r\n",
      "### 3. **应用场景**\r\n",
      "   - **学习辅助**：解答学科问题、生成学习笔记或模拟对话。\r\n",
      "   - **内容创作**：辅助写作、脚本生成、营销文案策划等。\r\n",
      "   - **日常助手**：提供生活建议、时间规划、趣味互动。\r\n",
      "   - **技术协作**：与开发者对接，优化代码逻辑或模型训练方案。\r\n",
      "\r\n",
      "### 4. **优势特点**\r\n",
      "   - **持续学习**：通过用户反馈与数据更新，保持知识时效性。\r\n",
      "   - **安全可控**：遵循伦理规范，避免生成有害内容。\r\n",
      "   - **灵活交互**：支持自由对话或指定任务（如“写一首关于春天的诗”）。\r\n",
      "\r\n",
      "### 5. **如何使用**\r\n",
      "   - 直接通过对话输入需求，或指定任务目标（如“生成Python爬虫代码”）。\r\n",
      "   - 结合Aistudio平台，可参与模型训练、部署或开源项目协作。\r\n",
      "\r\n",
      "期待通过我的能力，为您的学习、工作或生活提供高效支持！ 😊"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "host = \"0.0.0.0\"\n",
    "port = \"8180\"\n",
    "client = openai.Client(base_url=f\"http://{host}:{port}/v1\", api_key=\"null\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"null\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"你是Aistudio和文心大模型开发的智能助手，请介绍一下你自己.\"}\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta:\n",
    "        print(chunk.choices[0].delta.content, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T12:17:04.177492Z",
     "iopub.status.busy": "2025-07-06T12:17:04.177129Z",
     "iopub.status.idle": "2025-07-06T12:17:19.079569Z",
     "shell.execute_reply": "2025-07-06T12:17:19.078882Z",
     "shell.execute_reply.started": "2025-07-06T12:17:04.177471Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片描述：\r\n",
      "这张图片展示了飞桨自然语言处理开发库（PaddleNLP）的核心架构和功能模块，整体设计简洁清晰，采用蓝白配色，突出科技感。图片分为三个主要部分：\r\n",
      "\r\n",
      "### 1. **工业级预置任务（Taskflow）**\r\n",
      "   - **自然语言理解**：包含词法分析、文本纠错、情感分析、句法分析，覆盖文本基础处理与语义理解任务。\r\n",
      "   - **自然语言生成**：支持自动对联、智能写诗、生成式问答、开放域对话，体现NLP在创意性任务中的应用。\r\n",
      "\r\n",
      "### 2. **产业级模型库**\r\n",
      "   - **自研预训练模型**：列举了多个ERNIE系列模型（如ERNIE-1.0、ERNIE-2.0、ERNIE-Tiny等）及PLATO-2、SKEP等模型，体现飞桨在预训练模型领域的多样性。\r\n",
      "   - **覆盖全场景应用**：涵盖文本分类、匹配、生成、语义索引、小样本学习、文本图学习、信息抽取、翻译、知识关联、模型压缩等场景，展示模型库的广泛适用性。\r\n",
      "\r\n",
      "### 3. **文本领域核心API**\r\n",
      "   - 提供基础工具模块，包括数据管理（Data/Datasets）、嵌入（Embedding）、Transformer框架、序列到向量（Seq2Vec）、评估指标（Metrics）和损失函数（Losses），为开发者提供灵活的开发接口。\r\n",
      "\r\n",
      "### 底部标识\r\n",
      "   - 底部飞桨（PaddlePaddle）标志强化品牌一致性，体现飞桨作为开源深度学习平台的整体生态。\r\n",
      "\r\n",
      "**核心价值**：PaddleNLP通过预置任务、预训练模型和核心API，构建了从基础工具到行业应用的完整NLP解决方案，兼顾效率与灵活性，适合开发者快速落地自然语言处理项目。\r\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import base64\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "client = openai.Client(\n",
    "    base_url=\"http://0.0.0.0:8180/v1\",\n",
    "    api_key=\"null\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"null\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image('1.jpg')}\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"生成这张图片的描述\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"图片描述：\", end='', flush=True)\n",
    "for chunk in response:\n",
    "    if chunk.choices and chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end='', flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T17:34:50.762852Z",
     "iopub.status.busy": "2025-07-07T17:34:50.762356Z",
     "iopub.status.idle": "2025-07-07T17:34:52.391703Z",
     "shell.execute_reply": "2025-07-07T17:34:52.391041Z",
     "shell.execute_reply.started": "2025-07-07T17:34:50.762817Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T06:21:52.482043Z",
     "iopub.status.busy": "2025-07-07T06:21:52.481699Z",
     "iopub.status.idle": "2025-07-07T06:21:55.461737Z",
     "shell.execute_reply": "2025-07-07T06:21:55.461135Z",
     "shell.execute_reply.started": "2025-07-07T06:21:52.482022Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
      "To disable this warning, you can either:\r\n",
      "\t- Avoid using `tokenizers` before the fork if possible\r\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 成功添加 3 条记录到 symptoms 集合\r\n",
      "✅ 成功添加 3 条记录到 diseases 集合\r\n",
      "✅ 成功添加 3 条记录到 treatments 集合\r\n",
      "医学知识库初始化完成！\r\n"
     ]
    }
   ],
   "source": [
    "!python init_knowledge_base.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化失败，请先下载嵌入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T06:21:25.936880Z",
     "iopub.status.busy": "2025-07-07T06:21:25.936587Z",
     "iopub.status.idle": "2025-07-07T06:21:34.351247Z",
     "shell.execute_reply": "2025-07-07T06:21:34.350582Z",
     "shell.execute_reply.started": "2025-07-07T06:21:25.936861Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-07 14:21:26--  https://chroma-onnx-models.s3.amazonaws.com/all-MiniLM-L6-v2/onnx.tar.gz\r\n",
      "Resolving chroma-onnx-models.s3.amazonaws.com (chroma-onnx-models.s3.amazonaws.com)... 52.217.143.65, 3.5.27.174, 3.5.25.41, ...\r\n",
      "Connecting to chroma-onnx-models.s3.amazonaws.com (chroma-onnx-models.s3.amazonaws.com)|52.217.143.65|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 83178821 (79M) [application/x-gzip]\r\n",
      "Saving to: '/home/aistudio/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz'\r\n",
      "\r\n",
      "/home/aistudio/.cac 100%[===================>]  79.33M  13.7MB/s    in 7.0s    \r\n",
      "\r\n",
      "2025-07-07 14:21:34 (11.3 MB/s) - '/home/aistudio/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz' saved [83178821/83178821]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -O ~/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz \\\n",
    "    https://chroma-onnx-models.s3.amazonaws.com/all-MiniLM-L6-v2/onnx.tar.gz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
